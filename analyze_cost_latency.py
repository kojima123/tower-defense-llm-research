#!/usr/bin/env python3
"""
ã‚³ã‚¹ãƒˆãƒ»é…å»¶åˆ†æã‚·ã‚¹ãƒ†ãƒ 
å®Ÿå‹™çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè©•ä¾¡ï¼ˆtokens/ep, Â¥/100ep, ms/decisionï¼‰
"""
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class CostLatencyAnalyzer:
    """ã‚³ã‚¹ãƒˆãƒ»é…å»¶åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_dir: str = "."):
        self.project_dir = Path(project_dir)
        self.real_data_dir = self.project_dir / "runs" / "real"
        
        # OpenAI APIæ–™é‡‘è¨­å®š (2024å¹´ä¾¡æ ¼)
        self.openai_pricing = {
            "gpt-4o-mini": {
                "input": 0.00015,   # $0.15 per 1K tokens
                "output": 0.0006    # $0.60 per 1K tokens
            }
        }
        
        # ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ (æ¦‚ç®—)
        self.usd_to_jpy = 150
        
    def collect_performance_data(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print("â±ï¸  Collecting performance data...")
        
        data = {
            "conditions": {},
            "llm_usage": {
                "total_calls": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost_usd": 0.0,
                "average_response_time": 0.0,
                "response_times": []
            }
        }
        
        # CSVå½¢å¼ã®å®Ÿæ¸¬ãƒ­ã‚°ã‚’åé›†
        csv_files = list(self.real_data_dir.glob("**/*.csv"))
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                if len(df) == 0 or 'condition' not in df.columns:
                    continue
                
                condition = df['condition'].iloc[0]
                
                if condition not in data["conditions"]:
                    data["conditions"][condition] = {
                        "episodes": [],
                        "total_steps": 0,
                        "total_time": 0.0,
                        "decision_times": [],
                        "llm_calls": 0,
                        "files": []
                    }
                
                # åŸºæœ¬çµ±è¨ˆ
                data["conditions"][condition]["total_steps"] += len(df)
                data["conditions"][condition]["files"].append(str(csv_file.relative_to(self.project_dir)))
                
                # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
                if 'episode' in df.columns:
                    episodes = df['episode'].nunique()
                    data["conditions"][condition]["episodes"].append(episodes)
                
                # æ™‚é–“åˆ†æ
                if 'timestamp' in df.columns:
                    timestamps = df['timestamp'].values
                    if len(timestamps) > 1:
                        total_time = timestamps[-1] - timestamps[0]
                        data["conditions"][condition]["total_time"] += total_time
                        
                        # æ±ºå®šæ™‚é–“ (ã‚¹ãƒ†ãƒƒãƒ—é–“éš”)
                        step_times = np.diff(timestamps) * 1000  # ms
                        data["conditions"][condition]["decision_times"].extend(step_times.tolist())
                
                # LLMä½¿ç”¨çµ±è¨ˆ
                if 'llm_used' in df.columns:
                    llm_calls = df['llm_used'].sum()
                    data["conditions"][condition]["llm_calls"] += llm_calls
            
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {csv_file}: {e}")
                continue
        
        # LLMä»‹å…¥ãƒ­ã‚°ã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’åé›†
        self.collect_llm_cost_data(data)
        
        return data
    
    def collect_llm_cost_data(self, data: Dict[str, Any]):
        """LLMä»‹å…¥ãƒ­ã‚°ã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’åé›†"""
        print("ğŸ’° Collecting LLM cost data...")
        
        jsonl_files = list(self.real_data_dir.glob("**/*.jsonl"))
        
        for jsonl_file in jsonl_files:
            try:
                with jsonl_file.open('r') as f:
                    for line in f:
                        if line.strip():
                            log_data = json.loads(line)
                            
                            if log_data.get('type') == 'llm_intervention':
                                data["llm_usage"]["total_calls"] += 1
                                
                                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
                                usage = log_data.get('usage', {})
                                input_tokens = usage.get('prompt_tokens', 0)
                                output_tokens = usage.get('completion_tokens', 0)
                                
                                data["llm_usage"]["total_input_tokens"] += input_tokens
                                data["llm_usage"]["total_output_tokens"] += output_tokens
                                
                                # ã‚³ã‚¹ãƒˆè¨ˆç®—
                                input_cost = input_tokens / 1000 * self.openai_pricing["gpt-4o-mini"]["input"]
                                output_cost = output_tokens / 1000 * self.openai_pricing["gpt-4o-mini"]["output"]
                                data["llm_usage"]["total_cost_usd"] += input_cost + output_cost
                                
                                # å¿œç­”æ™‚é–“
                                response_time = log_data.get('response_time', 0)
                                if response_time > 0:
                                    data["llm_usage"]["response_times"].append(response_time)
            
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {jsonl_file}: {e}")
                continue
        
        # å¹³å‡å¿œç­”æ™‚é–“è¨ˆç®—
        if data["llm_usage"]["response_times"]:
            data["llm_usage"]["average_response_time"] = np.mean(data["llm_usage"]["response_times"])
    
    def calculate_cost_metrics(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚³ã‚¹ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—"""
        print("ğŸ“Š Calculating cost metrics...")
        
        metrics = {
            "per_episode": {},
            "per_100_episodes": {},
            "per_decision": {},
            "efficiency": {}
        }
        
        for condition, condition_data in data["conditions"].items():
            total_episodes = sum(condition_data["episodes"]) if condition_data["episodes"] else 1
            total_steps = condition_data["total_steps"]
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å½“ãŸã‚ŠæŒ‡æ¨™
            if total_episodes > 0:
                metrics["per_episode"][condition] = {
                    "steps": total_steps / total_episodes,
                    "time_seconds": condition_data["total_time"] / total_episodes,
                    "llm_calls": condition_data["llm_calls"] / total_episodes
                }
                
                # 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å½“ãŸã‚ŠæŒ‡æ¨™
                metrics["per_100_episodes"][condition] = {
                    "steps": (total_steps / total_episodes) * 100,
                    "time_minutes": (condition_data["total_time"] / total_episodes) * 100 / 60,
                    "llm_calls": (condition_data["llm_calls"] / total_episodes) * 100
                }
            
            # æ±ºå®šå½“ãŸã‚ŠæŒ‡æ¨™
            if total_steps > 0:
                avg_decision_time = np.mean(condition_data["decision_times"]) if condition_data["decision_times"] else 0
                
                metrics["per_decision"][condition] = {
                    "time_ms": avg_decision_time,
                    "llm_probability": condition_data["llm_calls"] / total_steps if total_steps > 0 else 0
                }
        
        # LLMç‰¹æœ‰ã®ã‚³ã‚¹ãƒˆæŒ‡æ¨™
        if data["llm_usage"]["total_calls"] > 0:
            total_tokens = data["llm_usage"]["total_input_tokens"] + data["llm_usage"]["total_output_tokens"]
            
            # ELM+LLMæ¡ä»¶ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å–å¾—
            elm_llm_episodes = sum(data["conditions"].get("elm_llm", {}).get("episodes", [0]))
            if elm_llm_episodes == 0:
                elm_llm_episodes = 1  # ã‚¼ãƒ­é™¤ç®—å›é¿
            
            metrics["llm_specific"] = {
                "tokens_per_call": total_tokens / data["llm_usage"]["total_calls"],
                "tokens_per_episode": total_tokens / elm_llm_episodes,
                "cost_usd_per_call": data["llm_usage"]["total_cost_usd"] / data["llm_usage"]["total_calls"],
                "cost_usd_per_episode": data["llm_usage"]["total_cost_usd"] / elm_llm_episodes,
                "cost_jpy_per_100_episodes": (data["llm_usage"]["total_cost_usd"] / elm_llm_episodes) * 100 * self.usd_to_jpy,
                "average_response_time_ms": data["llm_usage"]["average_response_time"] * 1000
            }
        else:
            metrics["llm_specific"] = {
                "tokens_per_call": 0,
                "tokens_per_episode": 0,
                "cost_usd_per_call": 0,
                "cost_usd_per_episode": 0,
                "cost_jpy_per_100_episodes": 0,
                "average_response_time_ms": 0
            }
        
        return metrics
    
    def create_cost_analysis_table(self, metrics: Dict[str, Any], output_path: str):
        """ã‚³ã‚¹ãƒˆåˆ†æè¡¨ã‚’ä½œæˆ"""
        print("ğŸ“‹ Creating cost analysis table...")
        
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        table_content = f"""# ã‚³ã‚¹ãƒˆãƒ»é…å»¶åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**åˆ†ææ—¥**: {current_date}  
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: å®Ÿæ¸¬ãƒ­ã‚°ã®ã¿ä½¿ç”¨  
**APIæ–™é‡‘**: OpenAI GPT-4o-mini (å…¥åŠ›: $0.15/1K tokens, å‡ºåŠ›: $0.60/1K tokens)  
**ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ**: $1 = Â¥{self.usd_to_jpy} (æ¦‚ç®—)

## ğŸ“Š æ¡ä»¶åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å½“ãŸã‚ŠæŒ‡æ¨™

| æ¡ä»¶ | ã‚¹ãƒ†ãƒƒãƒ—æ•° | å®Ÿè¡Œæ™‚é–“(ç§’) | LLMå‘¼ã³å‡ºã—å›æ•° |
|------|------------|--------------|-----------------|"""
        
        for condition, data in metrics["per_episode"].items():
            table_content += f"""
| {condition} | {data['steps']:.1f} | {data['time_seconds']:.2f} | {data['llm_calls']:.1f} |"""
        
        table_content += """

### 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å½“ãŸã‚ŠæŒ‡æ¨™

| æ¡ä»¶ | ç·ã‚¹ãƒ†ãƒƒãƒ—æ•° | å®Ÿè¡Œæ™‚é–“(åˆ†) | LLMå‘¼ã³å‡ºã—å›æ•° |
|------|--------------|--------------|-----------------|"""
        
        for condition, data in metrics["per_100_episodes"].items():
            table_content += f"""
| {condition} | {data['steps']:.0f} | {data['time_minutes']:.1f} | {data['llm_calls']:.0f} |"""
        
        table_content += """

### æ±ºå®šå½“ãŸã‚ŠæŒ‡æ¨™

| æ¡ä»¶ | æ±ºå®šæ™‚é–“(ms) | LLMä½¿ç”¨ç¢ºç‡ |
|------|--------------|-------------|"""
        
        for condition, data in metrics["per_decision"].items():
            table_content += f"""
| {condition} | {data['time_ms']:.2f} | {data['llm_probability']:.3f} |"""
        
        # LLMç‰¹æœ‰ã®ã‚³ã‚¹ãƒˆæŒ‡æ¨™
        if "llm_specific" in metrics:
            llm_metrics = metrics["llm_specific"]
            table_content += f"""

## ğŸ’° LLMä½¿ç”¨ã‚³ã‚¹ãƒˆåˆ†æ

### åŸºæœ¬æŒ‡æ¨™
- **ãƒˆãƒ¼ã‚¯ãƒ³/å‘¼ã³å‡ºã—**: {llm_metrics['tokens_per_call']:.1f} tokens
- **ãƒˆãƒ¼ã‚¯ãƒ³/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: {llm_metrics['tokens_per_episode']:.1f} tokens
- **å¹³å‡å¿œç­”æ™‚é–“**: {llm_metrics['average_response_time_ms']:.1f} ms

### ã‚³ã‚¹ãƒˆæŒ‡æ¨™
- **ã‚³ã‚¹ãƒˆ/å‘¼ã³å‡ºã—**: ${llm_metrics['cost_usd_per_call']:.4f} (Â¥{llm_metrics['cost_usd_per_call'] * self.usd_to_jpy:.2f})
- **ã‚³ã‚¹ãƒˆ/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: ${llm_metrics['cost_usd_per_episode']:.4f} (Â¥{llm_metrics['cost_usd_per_episode'] * self.usd_to_jpy:.2f})
- **ã‚³ã‚¹ãƒˆ/100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: ${llm_metrics['cost_usd_per_episode'] * 100:.2f} (Â¥{llm_metrics['cost_jpy_per_100_episodes']:.0f})

### å®Ÿå‹™çš„è©•ä¾¡
"""
            
            # å®Ÿå‹™çš„è©•ä¾¡
            cost_per_100ep = llm_metrics['cost_jpy_per_100_episodes']
            if cost_per_100ep < 100:
                cost_evaluation = "éå¸¸ã«ä½ã‚³ã‚¹ãƒˆ - å®Ÿç”¨çš„"
            elif cost_per_100ep < 500:
                cost_evaluation = "ä½ã‚³ã‚¹ãƒˆ - å®Ÿç”¨çš„"
            elif cost_per_100ep < 2000:
                cost_evaluation = "ä¸­ç¨‹åº¦ã‚³ã‚¹ãƒˆ - ç”¨é€”æ¬¡ç¬¬"
            else:
                cost_evaluation = "é«˜ã‚³ã‚¹ãƒˆ - æ…é‡ãªæ¤œè¨ãŒå¿…è¦"
            
            response_time = llm_metrics['average_response_time_ms']
            if response_time < 500:
                latency_evaluation = "ä½é…å»¶ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨é€”ã«é©ç”¨å¯èƒ½"
            elif response_time < 2000:
                latency_evaluation = "ä¸­ç¨‹åº¦é…å»¶ - æº–ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨é€”ã«é©ç”¨å¯èƒ½"
            else:
                latency_evaluation = "é«˜é…å»¶ - ãƒãƒƒãƒå‡¦ç†å‘ã‘"
            
            table_content += f"""
- **ã‚³ã‚¹ãƒˆè©•ä¾¡**: {cost_evaluation}
- **é…å»¶è©•ä¾¡**: {latency_evaluation}
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: {"é«˜" if cost_per_100ep < 500 and response_time < 1000 else "ä¸­" if cost_per_100ep < 2000 else "ä½"}
"""
        
        table_content += """

## ğŸ¯ æŠ•è³‡å¯¾åŠ¹æœåˆ†æ

### ROIæŒ‡æ¨™
"""
        
        # ROIåˆ†æï¼ˆELM+LLM vs ELMå˜ä½“ã®æ¯”è¼ƒï¼‰
        if "elm_llm" in metrics["per_episode"] and "elm_only" in metrics["per_episode"]:
            elm_llm_steps = metrics["per_episode"]["elm_llm"]["steps"]
            elm_only_steps = metrics["per_episode"]["elm_only"]["steps"]
            
            if "llm_specific" in metrics:
                cost_per_ep = metrics["llm_specific"]["cost_usd_per_episode"] * self.usd_to_jpy
                
                # åŠ¹ç‡æ”¹å–„ç‡
                if elm_only_steps > 0:
                    efficiency_improvement = (elm_llm_steps - elm_only_steps) / elm_only_steps * 100
                    
                    table_content += f"""
- **åŠ¹ç‡æ”¹å–„**: {efficiency_improvement:.1f}% (ELM+LLM vs ELMå˜ä½“)
- **æ”¹å–„ã‚³ã‚¹ãƒˆ**: Â¥{cost_per_ep:.2f}/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
- **æ”¹å–„ä¾¡å€¤**: {"é«˜" if efficiency_improvement > 10 else "ä¸­" if efficiency_improvement > 0 else "ä½"}
"""
        
        table_content += """

## ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬

### å¤§è¦æ¨¡é‹ç”¨æ™‚ã®ã‚³ã‚¹ãƒˆäºˆæ¸¬

| è¦æ¨¡ | ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° | äºˆæƒ³ã‚³ã‚¹ãƒˆ(Â¥) | å®Ÿè¡Œæ™‚é–“ |
|------|--------------|---------------|----------|"""
        
        if "llm_specific" in metrics:
            scales = [
                ("å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ", 1000, metrics["llm_specific"]["cost_jpy_per_100_episodes"] * 10),
                ("ä¸­è¦æ¨¡å®Ÿé¨“", 10000, metrics["llm_specific"]["cost_jpy_per_100_episodes"] * 100),
                ("å¤§è¦æ¨¡é‹ç”¨", 100000, metrics["llm_specific"]["cost_jpy_per_100_episodes"] * 1000)
            ]
            
            for scale_name, episodes, cost in scales:
                # å®Ÿè¡Œæ™‚é–“äºˆæ¸¬ï¼ˆELM+LLMæ¡ä»¶ã‹ã‚‰ï¼‰
                if "elm_llm" in metrics["per_episode"]:
                    time_per_ep = metrics["per_episode"]["elm_llm"]["time_seconds"]
                    total_time_hours = episodes * time_per_ep / 3600
                    time_text = f"{total_time_hours:.1f}æ™‚é–“"
                else:
                    time_text = "N/A"
                
                table_content += f"""
| {scale_name} | {episodes:,} | Â¥{cost:,.0f} | {time_text} |"""
        
        table_content += """

## ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: åˆæˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ãªã—
- âœ… **å®Ÿéš›ã®APIä½¿ç”¨**: OpenAIå®ŸAPIå‘¼ã³å‡ºã—è¨˜éŒ²
- âœ… **é€æ˜æ€§**: å…¨ã‚³ã‚¹ãƒˆè¨ˆç®—éç¨‹å…¬é–‹
- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(table_content)
        
        print(f"âœ… Cost analysis table saved: {output_path}")
    
    def create_cost_visualization(self, metrics: Dict[str, Any], output_path: str):
        """ã‚³ã‚¹ãƒˆå¯è¦–åŒ–ã‚’ä½œæˆ"""
        print("ğŸ“Š Creating cost visualization...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # æ¡ä»¶åˆ¥æ±ºå®šæ™‚é–“æ¯”è¼ƒ
        conditions = []
        decision_times = []
        
        for condition, data in metrics["per_decision"].items():
            conditions.append(condition)
            decision_times.append(data["time_ms"])
        
        if conditions:
            bars1 = ax1.bar(conditions, decision_times, alpha=0.7)
            ax1.set_title('Decision Time by Condition')
            ax1.set_ylabel('Time (ms)')
            ax1.tick_params(axis='x', rotation=45)
            
            # LLMæ¡ä»¶ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            for i, condition in enumerate(conditions):
                if 'llm' in condition.lower():
                    bars1[i].set_color('orange')
                    bars1[i].set_edgecolor('red')
                    bars1[i].set_linewidth(2)
        
        # LLMä½¿ç”¨ç¢ºç‡
        llm_probabilities = []
        for condition in conditions:
            prob = metrics["per_decision"][condition]["llm_probability"]
            llm_probabilities.append(prob)
        
        if conditions:
            ax2.bar(conditions, llm_probabilities, alpha=0.7, color='green')
            ax2.set_title('LLM Usage Probability by Condition')
            ax2.set_ylabel('Probability')
            ax2.set_ylim(0, 1)
            ax2.tick_params(axis='x', rotation=45)
        
        # ã‚³ã‚¹ãƒˆåˆ†æï¼ˆLLMãŒã‚ã‚‹å ´åˆï¼‰
        if "llm_specific" in metrics:
            llm_metrics = metrics["llm_specific"]
            
            # ã‚³ã‚¹ãƒˆå†…è¨³
            cost_categories = ['Per Call', 'Per Episode', 'Per 100 Episodes']
            cost_values = [
                llm_metrics['cost_usd_per_call'] * self.usd_to_jpy,
                llm_metrics['cost_usd_per_episode'] * self.usd_to_jpy,
                llm_metrics['cost_jpy_per_100_episodes']
            ]
            
            ax3.bar(cost_categories, cost_values, alpha=0.7, color='purple')
            ax3.set_title('LLM Cost Analysis (Â¥)')
            ax3.set_ylabel('Cost (JPY)')
            ax3.tick_params(axis='x', rotation=45)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°äºˆæ¸¬
            scales = [1000, 10000, 100000]
            costs = [llm_metrics['cost_jpy_per_100_episodes'] * (s/100) for s in scales]
            
            ax4.plot(scales, costs, marker='o', linewidth=2, markersize=8)
            ax4.set_title('Cost Scaling Prediction')
            ax4.set_xlabel('Episodes')
            ax4.set_ylabel('Total Cost (Â¥)')
            ax4.set_xscale('log')
            ax4.grid(True, alpha=0.3)
            
            # ã‚³ã‚¹ãƒˆé–¾å€¤ç·š
            ax4.axhline(y=10000, color='orange', linestyle='--', alpha=0.7, label='Â¥10,000')
            ax4.axhline(y=100000, color='red', linestyle='--', alpha=0.7, label='Â¥100,000')
            ax4.legend()
        else:
            ax3.text(0.5, 0.5, 'No LLM cost data available', ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('LLM Cost Analysis')
            
            ax4.text(0.5, 0.5, 'No LLM scaling data available', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Cost Scaling Prediction')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Cost visualization saved: {output_path}")
    
    def analyze_all(self):
        """å…¨ã‚³ã‚¹ãƒˆãƒ»é…å»¶åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸ’° Starting comprehensive cost and latency analysis...")
        print("=" * 70)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        data = self.collect_performance_data()
        
        # æŒ‡æ¨™è¨ˆç®—
        metrics = self.calculate_cost_metrics(data)
        
        # åˆ†æè¡¨ä½œæˆ
        self.create_cost_analysis_table(metrics, "cost_latency_analysis.md")
        
        # å¯è¦–åŒ–ä½œæˆ
        self.create_cost_visualization(metrics, "cost_latency_visualization.png")
        
        print("=" * 70)
        print("âœ… Cost and latency analysis completed")
        
        if "llm_specific" in metrics:
            llm_metrics = metrics["llm_specific"]
            print(f"ğŸ’° LLM cost per 100 episodes: Â¥{llm_metrics['cost_jpy_per_100_episodes']:.0f}")
            print(f"â±ï¸  Average response time: {llm_metrics['average_response_time_ms']:.1f} ms")
        
        print("ğŸ“ Generated files:")
        print("  - cost_latency_analysis.md")
        print("  - cost_latency_visualization.png")
        
        return metrics


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = CostLatencyAnalyzer()
    
    print("ğŸ’° Starting cost and latency analysis...")
    print("ğŸ“Š Analyzing real measurement data for practical impact...")
    
    # å…¨åˆ†æå®Ÿè¡Œ
    metrics = analyzer.analyze_all()
    
    print("\nğŸ‰ Cost and latency analysis complete!")


if __name__ == "__main__":
    main()
