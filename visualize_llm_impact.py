#!/usr/bin/env python3
"""
LLMä»‹å…¥åŠ¹æœã®å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
ä»‹å…¥â†’æ”¹å–„ã®ç›´æ¥è¨¼æ‹ ã‚’ç”Ÿæˆ
"""
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class LLMImpactVisualizer:
    """LLMä»‹å…¥åŠ¹æœã®å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_dir: str = "."):
        self.project_dir = Path(project_dir)
        self.real_data_dir = self.project_dir / "runs" / "real"
        
        # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
        plt.style.use('default')
        sns.set_palette("husl")
        
    def collect_llm_intervention_data(self) -> Dict[str, Any]:
        """LLMä»‹å…¥ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print("ğŸ¤– Collecting LLM intervention data...")
        
        intervention_data = {
            "interventions": [],
            "score_changes": [],
            "adoption_patterns": [],
            "time_series": []
        }
        
        # JSONLå½¢å¼ã®LLMä»‹å…¥ãƒ­ã‚°ã‚’æ¤œç´¢
        jsonl_files = list(self.real_data_dir.glob("**/*.jsonl"))
        
        for jsonl_file in jsonl_files:
            try:
                with jsonl_file.open('r') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('type') == 'llm_intervention':
                                intervention_data["interventions"].append(data)
                                
                                # æ¡ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
                                intervention_data["adoption_patterns"].append({
                                    'timestamp': data.get('timestamp', 0),
                                    'adopted': data.get('adopted', False),
                                    'confidence': data.get('confidence', 0),
                                    'episode': data.get('episode', 0),
                                    'step': data.get('step', 0)
                                })
            
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {jsonl_file}: {e}")
                continue
        
        return intervention_data
    
    def collect_score_trajectories(self) -> Dict[str, List[Dict]]:
        """æ¡ä»¶åˆ¥ã‚¹ã‚³ã‚¢è»Œè·¡ã‚’åé›†"""
        print("ğŸ“Š Collecting score trajectories...")
        
        trajectories = {
            "elm_only": [],
            "elm_llm": [],
            "rule_teacher": [],
            "random_teacher": []
        }
        
        csv_files = list(self.real_data_dir.glob("**/*.csv"))
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                if len(df) == 0 or 'condition' not in df.columns:
                    continue
                
                condition = df['condition'].iloc[0]
                if condition in trajectories:
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ¥ã‚¹ã‚³ã‚¢è»Œè·¡
                    for episode in df['episode'].unique():
                        episode_data = df[df['episode'] == episode].copy()
                        episode_data = episode_data.sort_values('step')
                        
                        trajectory = {
                            'episode': episode,
                            'steps': episode_data['step'].tolist(),
                            'scores': episode_data['score'].tolist(),
                            'seed': df['seed'].iloc[0] if 'seed' in df.columns else 0,
                            'file': str(csv_file.relative_to(self.project_dir))
                        }
                        
                        trajectories[condition].append(trajectory)
            
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {csv_file}: {e}")
                continue
        
        return trajectories
    
    def analyze_intervention_impact(self, intervention_data: Dict, trajectories: Dict) -> Dict[str, Any]:
        """ä»‹å…¥åŠ¹æœã‚’åˆ†æ"""
        print("ğŸ” Analyzing intervention impact...")
        
        analysis = {
            "total_interventions": len(intervention_data["interventions"]),
            "adoption_rate": 0.0,
            "score_improvements": [],
            "intervention_timing": [],
            "effectiveness_by_episode": {}
        }
        
        if not intervention_data["interventions"]:
            return analysis
        
        # æ¡ç”¨ç‡è¨ˆç®—
        adopted_count = sum(1 for pattern in intervention_data["adoption_patterns"] if pattern["adopted"])
        analysis["adoption_rate"] = (adopted_count / len(intervention_data["adoption_patterns"]) * 100) if intervention_data["adoption_patterns"] else 0
        
        # ELM+LLM vs ELMå˜ä½“ã®æ¯”è¼ƒ
        elm_llm_scores = []
        elm_only_scores = []
        
        for trajectory in trajectories.get("elm_llm", []):
            if trajectory["scores"]:
                elm_llm_scores.append(max(trajectory["scores"]))
        
        for trajectory in trajectories.get("elm_only", []):
            if trajectory["scores"]:
                elm_only_scores.append(max(trajectory["scores"]))
        
        if elm_llm_scores and elm_only_scores:
            llm_improvement = np.mean(elm_llm_scores) - np.mean(elm_only_scores)
            analysis["score_improvements"].append({
                "comparison": "elm_llm_vs_elm_only",
                "improvement": llm_improvement,
                "llm_mean": np.mean(elm_llm_scores),
                "elm_mean": np.mean(elm_only_scores),
                "llm_std": np.std(elm_llm_scores),
                "elm_std": np.std(elm_only_scores)
            })
        
        return analysis
    
    def create_intervention_timeline_plot(self, intervention_data: Dict, trajectories: Dict, output_path: str):
        """ä»‹å…¥ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–"""
        print("ğŸ“ˆ Creating intervention timeline plot...")
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
        
        # ä¸Šæ®µ: ã‚¹ã‚³ã‚¢è»Œè·¡æ¯”è¼ƒ
        colors = {'elm_only': '#1f77b4', 'elm_llm': '#ff7f0e', 'rule_teacher': '#2ca02c', 'random_teacher': '#d62728'}
        
        for condition, color in colors.items():
            if condition in trajectories and trajectories[condition]:
                all_scores = []
                all_steps = []
                
                for trajectory in trajectories[condition]:
                    if trajectory["scores"]:
                        all_scores.extend(trajectory["scores"])
                        all_steps.extend(trajectory["steps"])
                
                if all_scores:
                    # å¹³æ»‘åŒ–ã•ã‚ŒãŸè»Œè·¡
                    df_temp = pd.DataFrame({'step': all_steps, 'score': all_scores})
                    df_grouped = df_temp.groupby('step')['score'].mean().reset_index()
                    
                    ax1.plot(df_grouped['step'], df_grouped['score'], 
                            color=color, label=condition, linewidth=2, alpha=0.8)
        
        ax1.set_xlabel('Step')
        ax1.set_ylabel('Score')
        ax1.set_title('Score Trajectories by Condition')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ä¸‹æ®µ: LLMä»‹å…¥åŠ¹æœ
        if intervention_data["adoption_patterns"]:
            adoption_df = pd.DataFrame(intervention_data["adoption_patterns"])
            
            # æ¡ç”¨ãƒ»éæ¡ç”¨ã®åˆ†å¸ƒ
            adopted = adoption_df[adoption_df['adopted'] == True]
            not_adopted = adoption_df[adoption_df['adopted'] == False]
            
            ax2.scatter(adopted['step'], adopted['episode'], 
                       color='green', s=100, alpha=0.7, label=f'Adopted ({len(adopted)})')
            ax2.scatter(not_adopted['step'], not_adopted['episode'], 
                       color='red', s=100, alpha=0.7, label=f'Not Adopted ({len(not_adopted)})')
            
            ax2.set_xlabel('Step')
            ax2.set_ylabel('Episode')
            ax2.set_title('LLM Intervention Adoption Pattern')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        else:
            ax2.text(0.5, 0.5, 'No LLM intervention data available', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=14)
            ax2.set_title('LLM Intervention Analysis')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Timeline plot saved: {output_path}")
    
    def create_performance_comparison_plot(self, trajectories: Dict, analysis: Dict, output_path: str):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒå¯è¦–åŒ–"""
        print("ğŸ“Š Creating performance comparison plot...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # æ¡ä»¶åˆ¥æœ€çµ‚ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        condition_scores = {}
        for condition, trajs in trajectories.items():
            scores = [max(traj["scores"]) if traj["scores"] else 0 for traj in trajs]
            condition_scores[condition] = scores
        
        if condition_scores:
            # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
            data_for_box = []
            labels_for_box = []
            for condition, scores in condition_scores.items():
                data_for_box.extend(scores)
                labels_for_box.extend([condition] * len(scores))
            
            df_box = pd.DataFrame({'condition': labels_for_box, 'score': data_for_box})
            sns.boxplot(data=df_box, x='condition', y='score', ax=ax1)
            ax1.set_title('Final Score Distribution by Condition')
            ax1.tick_params(axis='x', rotation=45)
            
            # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
            sns.violinplot(data=df_box, x='condition', y='score', ax=ax2)
            ax2.set_title('Score Distribution Density')
            ax2.tick_params(axis='x', rotation=45)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
        if condition_scores:
            conditions = list(condition_scores.keys())
            means = [np.mean(scores) if scores else 0 for scores in condition_scores.values()]
            stds = [np.std(scores) if len(scores) > 1 else 0 for scores in condition_scores.values()]
            
            bars = ax3.bar(conditions, means, yerr=stds, capsize=5, alpha=0.7)
            ax3.set_title('Mean Score with Standard Deviation')
            ax3.set_ylabel('Score')
            ax3.tick_params(axis='x', rotation=45)
            
            # æœ€é«˜å€¤ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            max_idx = np.argmax(means)
            bars[max_idx].set_color('gold')
            bars[max_idx].set_edgecolor('orange')
            bars[max_idx].set_linewidth(2)
        
        # LLMåŠ¹æœåˆ†æ
        if analysis["score_improvements"]:
            improvement = analysis["score_improvements"][0]
            
            categories = ['ELM Only', 'ELM + LLM']
            values = [improvement["elm_mean"], improvement["llm_mean"]]
            errors = [improvement["elm_std"], improvement["llm_std"]]
            
            bars = ax4.bar(categories, values, yerr=errors, capsize=5, 
                          color=['#1f77b4', '#ff7f0e'], alpha=0.7)
            ax4.set_title(f'LLM Impact Analysis\n(Improvement: {improvement["improvement"]:.1f} points)')
            ax4.set_ylabel('Average Score')
            
            # æ”¹å–„åŠ¹æœã‚’çŸ¢å°ã§è¡¨ç¤º
            if improvement["improvement"] > 0:
                ax4.annotate('', xy=(1, improvement["llm_mean"]), xytext=(0, improvement["elm_mean"]),
                           arrowprops=dict(arrowstyle='->', color='green', lw=2))
                ax4.text(0.5, max(values) * 0.9, f'+{improvement["improvement"]:.1f}', 
                        ha='center', va='center', color='green', fontweight='bold', fontsize=12)
        else:
            ax4.text(0.5, 0.5, 'No LLM improvement data available', 
                    ha='center', va='center', transform=ax4.transAxes, fontsize=12)
            ax4.set_title('LLM Impact Analysis')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Performance comparison plot saved: {output_path}")
    
    def create_adoption_analysis_plot(self, intervention_data: Dict, output_path: str):
        """æ¡ç”¨ç‡åˆ†æå¯è¦–åŒ–"""
        print("ğŸ¯ Creating adoption analysis plot...")
        
        if not intervention_data["adoption_patterns"]:
            # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            ax.text(0.5, 0.5, 'No LLM intervention data available\nRun experiments with --teachers elm_llm to generate data', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=14)
            ax.set_title('LLM Intervention Adoption Analysis')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ… Placeholder adoption plot saved: {output_path}")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))
        
        adoption_df = pd.DataFrame(intervention_data["adoption_patterns"])
        
        # æ¡ç”¨ç‡å††ã‚°ãƒ©ãƒ•
        adopted_count = adoption_df['adopted'].sum()
        not_adopted_count = len(adoption_df) - adopted_count
        
        if adopted_count + not_adopted_count > 0:
            ax1.pie([adopted_count, not_adopted_count], 
                   labels=[f'Adopted ({adopted_count})', f'Not Adopted ({not_adopted_count})'],
                   colors=['#2ca02c', '#d62728'], autopct='%1.1f%%', startangle=90)
            ax1.set_title(f'LLM Intervention Adoption Rate\n(Total: {len(adoption_df)} interventions)')
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ¥æ¡ç”¨ç‡
        if 'episode' in adoption_df.columns:
            episode_adoption = adoption_df.groupby('episode')['adopted'].agg(['count', 'sum']).reset_index()
            episode_adoption['adoption_rate'] = episode_adoption['sum'] / episode_adoption['count'] * 100
            
            ax2.bar(episode_adoption['episode'], episode_adoption['adoption_rate'], alpha=0.7)
            ax2.set_xlabel('Episode')
            ax2.set_ylabel('Adoption Rate (%)')
            ax2.set_title('Adoption Rate by Episode')
            ax2.set_ylim(0, 100)
        
        # ä¿¡é ¼åº¦åˆ†å¸ƒ
        if 'confidence' in adoption_df.columns:
            adopted_conf = adoption_df[adoption_df['adopted'] == True]['confidence']
            not_adopted_conf = adoption_df[adoption_df['adopted'] == False]['confidence']
            
            if len(adopted_conf) > 0:
                ax3.hist(adopted_conf, bins=10, alpha=0.7, label='Adopted', color='green')
            if len(not_adopted_conf) > 0:
                ax3.hist(not_adopted_conf, bins=10, alpha=0.7, label='Not Adopted', color='red')
            
            ax3.set_xlabel('Confidence Score')
            ax3.set_ylabel('Frequency')
            ax3.set_title('Confidence Distribution')
            ax3.legend()
        
        # æ™‚ç³»åˆ—æ¡ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'timestamp' in adoption_df.columns:
            adoption_df_sorted = adoption_df.sort_values('timestamp')
            adoption_df_sorted['cumulative_adoption_rate'] = adoption_df_sorted['adopted'].expanding().mean() * 100
            
            ax4.plot(range(len(adoption_df_sorted)), adoption_df_sorted['cumulative_adoption_rate'], 
                    marker='o', linewidth=2, markersize=4)
            ax4.set_xlabel('Intervention Number')
            ax4.set_ylabel('Cumulative Adoption Rate (%)')
            ax4.set_title('Adoption Rate Over Time')
            ax4.set_ylim(0, 100)
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Adoption analysis plot saved: {output_path}")
    
    def generate_impact_report(self, analysis: Dict, output_path: str):
        """LLMåŠ¹æœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“ Generating LLM impact report...")
        
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report_content = f"""# LLMä»‹å…¥åŠ¹æœåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {current_date}  
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: å®Ÿæ¸¬ãƒ­ã‚°ã®ã¿ä½¿ç”¨

## ğŸ“Š ä»‹å…¥çµ±è¨ˆã‚µãƒãƒªãƒ¼

- **ç·ä»‹å…¥å›æ•°**: {analysis['total_interventions']}å›
- **æ¡ç”¨ç‡**: {analysis['adoption_rate']:.1f}%
- **åˆ†æå¯¾è±¡**: å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿0ä»¶ï¼‰

## ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœ

"""
        
        if analysis["score_improvements"]:
            improvement = analysis["score_improvements"][0]
            report_content += f"""### ELM+LLM vs ELMå˜ä½“æ¯”è¼ƒ

| æŒ‡æ¨™ | ELMå˜ä½“ | ELM+LLM | æ”¹å–„åŠ¹æœ |
|------|---------|---------|----------|
| å¹³å‡ã‚¹ã‚³ã‚¢ | {improvement['elm_mean']:.2f} Â± {improvement['elm_std']:.2f} | {improvement['llm_mean']:.2f} Â± {improvement['llm_std']:.2f} | {improvement['improvement']:.2f} |
| æ”¹å–„ç‡ | - | - | {(improvement['improvement']/improvement['elm_mean']*100):.1f}% |

**çµè«–**: {'LLMä»‹å…¥ã«ã‚ˆã‚Šæœ‰æ„ãªæ”¹å–„' if improvement['improvement'] > 0 else 'LLMä»‹å…¥ã«ã‚ˆã‚‹æ”¹å–„ã¯é™å®šçš„'}
"""
        else:
            report_content += """### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã§ã¯ååˆ†ãªLLMä»‹å…¥ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚
ã‚ˆã‚Šå¤šãã®ELM+LLMå®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```bash
python run_experiment_cli_fixed.py run --teachers elm_llm --episodes 20 --seeds 42 123 456
```
"""
        
        report_content += f"""

## ğŸ“ˆ å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«

1. **ä»‹å…¥ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³**: [`llm_intervention_timeline.png`](./llm_intervention_timeline.png)
2. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ**: [`llm_performance_comparison.png`](./llm_performance_comparison.png)  
3. **æ¡ç”¨ç‡åˆ†æ**: [`llm_adoption_analysis.png`](./llm_adoption_analysis.png)

## ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: åˆæˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ãªã—
- âœ… **é€æ˜æ€§**: å…¨ä»‹å…¥ãƒ­ã‚°å…¬é–‹
- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“

## ğŸ’¡ æ¨å¥¨äº‹é …

1. **ãƒ‡ãƒ¼ã‚¿å……å®Ÿ**: ã‚ˆã‚Šå¤šãã®ELM+LLMå®Ÿé¨“å®Ÿè¡Œ
2. **é•·æœŸè©•ä¾¡**: è¤‡æ•°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã®åŠ¹æœæ¸¬å®š
3. **ã‚³ã‚¹ãƒˆåˆ†æ**: APIä½¿ç”¨æ–™ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è©•ä¾¡

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… Impact report saved: {output_path}")
    
    def visualize_all(self):
        """å…¨ã¦ã®å¯è¦–åŒ–ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Starting comprehensive LLM impact visualization...")
        print("=" * 70)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        intervention_data = self.collect_llm_intervention_data()
        trajectories = self.collect_score_trajectories()
        
        # åˆ†æå®Ÿè¡Œ
        analysis = self.analyze_intervention_impact(intervention_data, trajectories)
        
        # å¯è¦–åŒ–ç”Ÿæˆ
        self.create_intervention_timeline_plot(
            intervention_data, trajectories, 
            "llm_intervention_timeline.png"
        )
        
        self.create_performance_comparison_plot(
            trajectories, analysis,
            "llm_performance_comparison.png"
        )
        
        self.create_adoption_analysis_plot(
            intervention_data,
            "llm_adoption_analysis.png"
        )
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_impact_report(analysis, "llm_impact_report.md")
        
        print("=" * 70)
        print("âœ… LLM impact visualization completed")
        print(f"ğŸ“Š Analyzed {analysis['total_interventions']} interventions")
        print(f"ğŸ¯ Adoption rate: {analysis['adoption_rate']:.1f}%")
        print("ğŸ“ˆ Generated 3 visualization files + 1 report")
        
        return analysis


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    visualizer = LLMImpactVisualizer()
    
    print("ğŸ¤– Starting LLM intervention impact visualization...")
    print("ğŸ“Š Analyzing real measurement data for LLM effectiveness...")
    
    # å…¨å¯è¦–åŒ–å®Ÿè¡Œ
    analysis = visualizer.visualize_all()
    
    print("\nğŸ‰ LLM impact analysis complete!")
    print("ğŸ“ Generated files:")
    print("  - llm_intervention_timeline.png")
    print("  - llm_performance_comparison.png") 
    print("  - llm_adoption_analysis.png")
    print("  - llm_impact_report.md")


if __name__ == "__main__":
    main()
