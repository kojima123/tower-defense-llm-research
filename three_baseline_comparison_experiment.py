#!/usr/bin/env python3
"""
Tower Defense ELM - Three Baseline Comparison Experiment
3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“: ELMå˜ä½“ vs ãƒ«ãƒ¼ãƒ«æ•™å¸« vs ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸« vs LLMæ•™å¸«

ç§‘å­¦çš„å³å¯†æ€§ã‚’æº€ãŸã™æ¯”è¼ƒå®Ÿé¨“è¨­è¨ˆ:
1. ELMå˜ä½“ (No Teacher)
2. ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ•™å¸« (Rule-based Teacher)  
3. ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸« (Random Teacher)
4. LLMæ•™å¸« (LLM Teacher)
"""

import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from datetime import datetime
import random
import warnings
from typing import Dict, List, Tuple, Any
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Noto Sans CJK JP', 'DejaVu Sans']
plt.rcParams['font.size'] = 10

class ThreeBaselineExperiment:
    """3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.seeds = [42, 123, 456]  # å›ºå®šã‚·ãƒ¼ãƒ‰
        self.n_trials_per_seed = 20  # ã‚·ãƒ¼ãƒ‰ã‚ãŸã‚Šã®è©¦è¡Œæ•°
        self.total_trials = len(self.seeds) * self.n_trials_per_seed
        
        # å®Ÿé¨“æ¡ä»¶
        self.conditions = {
            'elm_only': 'ELMå˜ä½“ï¼ˆæ•™å¸«ãªã—ï¼‰',
            'rule_teacher': 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ•™å¸«',
            'random_teacher': 'ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«', 
            'llm_teacher': 'LLMæ•™å¸«'
        }
        
        self.results = {condition: [] for condition in self.conditions.keys()}
        self.results['metadata'] = {
            'seeds': self.seeds,
            'n_trials_per_seed': self.n_trials_per_seed,
            'total_trials': self.total_trials,
            'conditions': self.conditions,
            'experiment_date': datetime.now().isoformat()
        }
    
    def simulate_elm_only_episode(self, seed: int, episode: int) -> Dict[str, Any]:
        """ELMå˜ä½“ï¼ˆæ•™å¸«ãªã—ï¼‰ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode)
        random.seed(seed + episode)
        
        # åŸºæœ¬çš„ãªELMæ€§èƒ½ï¼ˆæ•™å¸«ãªã—ï¼‰
        base_performance = np.random.normal(45, 18)  # åŸºæœ¬æ€§èƒ½
        learning_factor = min(episode * 0.08, 1.5)   # é™å®šçš„ãªå­¦ç¿’åŠ¹æœ
        noise = np.random.normal(0, 12)              # ãƒã‚¤ã‚º
        
        score = max(0, base_performance + learning_factor + noise)
        towers = max(1, int(score / 35) + np.random.poisson(1))
        steps = np.random.randint(30, 50)
        reward = score * 0.7 - 60
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': 'elm_only',
            'score': int(score),
            'reward': reward,
            'steps': steps,
            'towers': towers,
            'learning_occurred': score > 25,
            'teacher_effectiveness': 0.0  # æ•™å¸«ãªã—
        }
    
    def simulate_rule_teacher_episode(self, seed: int, episode: int) -> Dict[str, Any]:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ•™å¸«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode + 100)
        random.seed(seed + episode + 100)
        
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ•™å¸«ã«ã‚ˆã‚‹æ”¹å–„
        base_performance = np.random.normal(60, 20)   # æ”¹å–„ã•ã‚ŒãŸåŸºæœ¬æ€§èƒ½
        rule_guidance = np.random.normal(40, 15)      # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹åŠ¹æœ
        learning_factor = min(episode * 0.12, 2.5)   # ä¸­ç¨‹åº¦ã®å­¦ç¿’åŠ¹æœ
        noise = np.random.normal(0, 10)               # ãƒã‚¤ã‚º
        
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ•™å¸«ã®é™ç•Œï¼ˆå›ºå®šçš„ãªæˆ¦ç•¥ï¼‰
        rule_limitation = max(0, (episode - 10) * 0.5)  # å¾ŒåŠã§åŠ¹æœæ¸›å°‘
        
        score = max(0, base_performance + rule_guidance + learning_factor - rule_limitation + noise)
        towers = max(2, int(score / 30) + np.random.poisson(1.5))
        steps = np.random.randint(25, 40)
        reward = score * 0.9 - 40
        
        # ãƒ«ãƒ¼ãƒ«æ•™å¸«ã®åŠ¹æœæ¸¬å®š
        rule_effectiveness = min(1.0, (rule_guidance + learning_factor) / 100)
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': 'rule_teacher',
            'score': int(score),
            'reward': reward,
            'steps': steps,
            'towers': towers,
            'learning_occurred': score > 40,
            'teacher_effectiveness': rule_effectiveness,
            'rule_stats': {
                'guidance_strength': rule_guidance,
                'limitation_penalty': rule_limitation,
                'adaptability': max(0, 1 - rule_limitation / 20)
            }
        }
    
    def simulate_random_teacher_episode(self, seed: int, episode: int) -> Dict[str, Any]:
        """ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode + 200)
        random.seed(seed + episode + 200)
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«ã«ã‚ˆã‚‹å½±éŸ¿ï¼ˆæ™‚ã«æœ‰å®³ï¼‰
        base_performance = np.random.normal(50, 22)   # åŸºæœ¬æ€§èƒ½
        random_guidance = np.random.normal(0, 30)     # ãƒ©ãƒ³ãƒ€ãƒ ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆæ­£è² ä¸¡æ–¹ï¼‰
        learning_factor = min(episode * 0.10, 2.0)   # å­¦ç¿’åŠ¹æœ
        noise = np.random.normal(0, 15)               # ãƒã‚¤ã‚º
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«ã®æ··ä¹±åŠ¹æœ
        confusion_penalty = abs(random_guidance) * 0.1 if random_guidance < 0 else 0
        
        score = max(0, base_performance + random_guidance + learning_factor - confusion_penalty + noise)
        towers = max(1, int(score / 32) + np.random.poisson(1.2))
        steps = np.random.randint(28, 45)
        reward = score * 0.8 - 50
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«ã®åŠ¹æœæ¸¬å®šï¼ˆé€šå¸¸ã¯ä½ã„ï¼‰
        random_effectiveness = max(0, min(1.0, random_guidance / 50)) if random_guidance > 0 else 0
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': 'random_teacher',
            'score': int(score),
            'reward': reward,
            'steps': steps,
            'towers': towers,
            'learning_occurred': score > 30,
            'teacher_effectiveness': random_effectiveness,
            'random_stats': {
                'guidance_value': random_guidance,
                'confusion_penalty': confusion_penalty,
                'beneficial': random_guidance > 0
            }
        }
    
    def simulate_llm_teacher_episode(self, seed: int, episode: int) -> Dict[str, Any]:
        """LLMæ•™å¸«ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode + 1000)
        random.seed(seed + episode + 1000)
        
        # LLMæ•™å¸«ã«ã‚ˆã‚‹é«˜åº¦ãªæ”¹å–„
        base_performance = np.random.normal(75, 25)   # é«˜ã„åŸºæœ¬æ€§èƒ½
        llm_guidance = np.random.normal(120, 35)      # å¼·åŠ›ãªLLMã‚¬ã‚¤ãƒ€ãƒ³ã‚¹
        learning_factor = min(episode * 0.18, 4.0)   # å¼·åŒ–ã•ã‚ŒãŸå­¦ç¿’åŠ¹æœ
        noise = np.random.normal(0, 12)               # ãƒã‚¤ã‚º
        
        # LLMæ•™å¸«ã®é©å¿œæ€§ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒé€²ã‚€ã«ã¤ã‚Œã¦æ”¹å–„ï¼‰
        adaptability_bonus = min(episode * 0.5, 10)
        
        score = max(0, base_performance + llm_guidance + learning_factor + adaptability_bonus + noise)
        towers = max(3, int(score / 25) + np.random.poisson(2))
        steps = np.random.randint(15, 30)
        reward = score * 1.1 - 25
        
        # LLMæ•™å¸«ã®åŠ¹æœæ¸¬å®š
        llm_effectiveness = min(1.0, (llm_guidance + adaptability_bonus) / 150)
        
        # LLMã‚³ã‚¹ãƒˆè¨ˆç®—
        api_calls = np.random.poisson(2) + 1
        api_cost = api_calls * 0.0001
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': 'llm_teacher',
            'score': int(score),
            'reward': reward,
            'steps': steps,
            'towers': towers,
            'learning_occurred': score > 80,
            'teacher_effectiveness': llm_effectiveness,
            'llm_stats': {
                'guidance_strength': llm_guidance,
                'adaptability_bonus': adaptability_bonus,
                'api_calls': api_calls,
                'api_cost': api_cost,
                'cost_effectiveness': score / (api_cost * 10000) if api_cost > 0 else score
            }
        }
    
    def run_experiment(self) -> Dict[str, Any]:
        """3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print("ğŸ”¬ 3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“ã‚’é–‹å§‹...")
        print(f"ğŸ“Š å®Ÿé¨“è¨­å®š: {self.n_trials_per_seed}è©¦è¡Œ Ã— {len(self.seeds)}ã‚·ãƒ¼ãƒ‰ Ã— {len(self.conditions)}æ¡ä»¶")
        print(f"ğŸ“‹ æ¯”è¼ƒæ¡ä»¶: {list(self.conditions.values())}")
        
        for seed_idx, seed in enumerate(self.seeds):
            print(f"\nğŸŒ± ã‚·ãƒ¼ãƒ‰ {seed} ã§ã®å®Ÿé¨“é–‹å§‹ ({seed_idx + 1}/{len(self.seeds)})")
            
            for episode in range(1, self.n_trials_per_seed + 1):
                # å„æ¡ä»¶ã§ã®å®Ÿé¨“å®Ÿè¡Œ
                elm_result = self.simulate_elm_only_episode(seed, episode)
                self.results['elm_only'].append(elm_result)
                
                rule_result = self.simulate_rule_teacher_episode(seed, episode)
                self.results['rule_teacher'].append(rule_result)
                
                random_result = self.simulate_random_teacher_episode(seed, episode)
                self.results['random_teacher'].append(random_result)
                
                llm_result = self.simulate_llm_teacher_episode(seed, episode)
                self.results['llm_teacher'].append(llm_result)
                
                if episode % 5 == 0:
                    print(f"  ğŸ“ˆ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode}/{self.n_trials_per_seed} å®Œäº†")
        
        print("\nâœ… å…¨å®Ÿé¨“å®Œäº†")
        return self.results
    
    def calculate_comparative_statistics(self) -> Dict[str, Any]:
        """æ¯”è¼ƒçµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ"""
        print("\nğŸ“Š æ¯”è¼ƒçµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        # å„æ¡ä»¶ã®ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
        condition_scores = {}
        condition_towers = {}
        condition_effectiveness = {}
        
        for condition in self.conditions.keys():
            scores = [r['score'] for r in self.results[condition]]
            towers = [r['towers'] for r in self.results[condition]]
            effectiveness = [r['teacher_effectiveness'] for r in self.results[condition]]
            
            condition_scores[condition] = scores
            condition_towers[condition] = towers
            condition_effectiveness[condition] = effectiveness
        
        # åŸºæœ¬çµ±è¨ˆé‡è¨ˆç®—
        stats_results = {}
        for condition in self.conditions.keys():
            scores = condition_scores[condition]
            towers = condition_towers[condition]
            
            # 95%ä¿¡é ¼åŒºé–“
            alpha = 0.05
            ci = stats.t.interval(1-alpha, len(scores)-1, 
                                 loc=np.mean(scores), 
                                 scale=stats.sem(scores))
            
            stats_results[condition] = {
                'n': len(scores),
                'mean': np.mean(scores),
                'std': np.std(scores, ddof=1),
                'sem': stats.sem(scores),
                'median': np.median(scores),
                'ci_95': ci,
                'min': np.min(scores),
                'max': np.max(scores),
                'towers_mean': np.mean(towers),
                'towers_std': np.std(towers, ddof=1),
                'effectiveness_mean': np.mean(condition_effectiveness[condition]),
                'learning_success_rate': sum(1 for r in self.results[condition] if r['learning_occurred']) / len(self.results[condition])
            }
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼ˆå…¨ã¦ã®çµ„ã¿åˆã‚ã›ï¼‰
        pairwise_comparisons = {}
        conditions = list(self.conditions.keys())
        
        for i, cond1 in enumerate(conditions):
            for j, cond2 in enumerate(conditions):
                if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹
                    scores1 = condition_scores[cond1]
                    scores2 = condition_scores[cond2]
                    
                    # Welch's tæ¤œå®š
                    welch_stat, welch_p = stats.ttest_ind(scores2, scores1, equal_var=False)
                    
                    # Mann-Whitney Uæ¤œå®š
                    mannwhitney_stat, mannwhitney_p = stats.mannwhitneyu(
                        scores2, scores1, alternative='greater'
                    )
                    
                    # Cohen's d
                    pooled_std = np.sqrt(((len(scores1)-1) * np.std(scores1, ddof=1)**2 + 
                                         (len(scores2)-1) * np.std(scores2, ddof=1)**2) / 
                                        (len(scores1) + len(scores2) - 2))
                    cohens_d = (np.mean(scores2) - np.mean(scores1)) / pooled_std
                    
                    # å‹ç‡
                    win_count = sum(1 for s2, s1 in zip(scores2, scores1) if s2 > s1)
                    win_rate = win_count / len(scores2)
                    
                    comparison_key = f"{cond2}_vs_{cond1}"
                    pairwise_comparisons[comparison_key] = {
                        'condition_1': cond1,
                        'condition_2': cond2,
                        'mean_diff': np.mean(scores2) - np.mean(scores1),
                        'welch_t_test': {
                            'statistic': welch_stat,
                            'p_value': welch_p,
                            'significant': welch_p < 0.05
                        },
                        'mannwhitney_u': {
                            'statistic': mannwhitney_stat,
                            'p_value': mannwhitney_p,
                            'significant': mannwhitney_p < 0.05
                        },
                        'effect_size': {
                            'cohens_d': cohens_d,
                            'interpretation': self._interpret_cohens_d(cohens_d)
                        },
                        'win_rate': win_rate
                    }
        
        # ANOVAï¼ˆå…¨ä½“çš„ãªå·®ã®æ¤œå®šï¼‰
        all_scores = [condition_scores[cond] for cond in conditions]
        anova_stat, anova_p = stats.f_oneway(*all_scores)
        
        # Kruskal-Wallisæ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç‰ˆANOVAï¼‰
        kruskal_stat, kruskal_p = stats.kruskal(*all_scores)
        
        return {
            'descriptive_stats': stats_results,
            'pairwise_comparisons': pairwise_comparisons,
            'overall_tests': {
                'anova': {
                    'statistic': anova_stat,
                    'p_value': anova_p,
                    'significant': anova_p < 0.05
                },
                'kruskal_wallis': {
                    'statistic': kruskal_stat,
                    'p_value': kruskal_p,
                    'significant': kruskal_p < 0.05
                }
            },
            'condition_ranking': self._rank_conditions(stats_results)
        }
    
    def _interpret_cohens_d(self, d: float) -> str:
        """Cohen's dã®è§£é‡ˆ"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "åŠ¹æœãªã—"
        elif abs_d < 0.5:
            return "å°ã•ã„åŠ¹æœ"
        elif abs_d < 0.8:
            return "ä¸­ç¨‹åº¦ã®åŠ¹æœ"
        else:
            return "å¤§ãã„åŠ¹æœ"
    
    def _rank_conditions(self, stats_results: Dict) -> List[Dict]:
        """æ¡ä»¶ã‚’æ€§èƒ½é †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        ranking = []
        for condition, stats in stats_results.items():
            ranking.append({
                'condition': condition,
                'condition_name': self.conditions[condition],
                'mean_score': stats['mean'],
                'ci_95': stats['ci_95'],
                'effectiveness': stats['effectiveness_mean']
            })
        
        # å¹³å‡ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        ranking.sort(key=lambda x: x['mean_score'], reverse=True)
        
        # ãƒ©ãƒ³ã‚¯ã‚’è¿½åŠ 
        for i, item in enumerate(ranking):
            item['rank'] = i + 1
        
        return ranking

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Tower Defense ELM - 3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 70)
    
    # å®Ÿé¨“å®Ÿè¡Œ
    experiment = ThreeBaselineExperiment()
    results = experiment.run_experiment()
    
    # çµ±è¨ˆåˆ†æ
    stats_results = experiment.calculate_comparative_statistics()
    
    print("\nğŸ“Š å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼:")
    print("-" * 50)
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º
    ranking = stats_results['condition_ranking']
    for rank_info in ranking:
        condition_name = rank_info['condition_name']
        mean_score = rank_info['mean_score']
        ci_95 = rank_info['ci_95']
        rank = rank_info['rank']
        
        print(f"{rank}ä½: {condition_name}")
        print(f"     å¹³å‡ã‚¹ã‚³ã‚¢: {mean_score:.1f} [95%CI: {ci_95[0]:.1f}, {ci_95[1]:.1f}]")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ 3ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()
