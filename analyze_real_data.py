#!/usr/bin/env python3
"""
å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿å°‚ç”¨çµ±è¨ˆåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡ä½¿ç”¨ã›ãšã€å®Ÿéš›ã®å®Ÿé¨“ãƒ­ã‚°ã®ã¿ã‹ã‚‰ç§‘å­¦çš„åˆ†æã‚’å®Ÿè¡Œ
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import argparse
from typing import Dict, List, Tuple, Any
from scipy import stats as scipy_stats
import warnings
warnings.filterwarnings('ignore')


class RealDataAnalyzer:
    """å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿å°‚ç”¨åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.conditions = ["elm_only", "rule_teacher", "random_teacher", "elm_llm"]
        self.data = {}
        self.summary_stats = {}
        
    def load_real_data(self) -> bool:
        """å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“Š Loading real experimental data...")
        
        for condition in self.conditions:
            condition_dir = self.base_dir / condition
            if not condition_dir.exists():
                print(f"   âš ï¸  Warning: {condition} directory not found")
                continue
            
            condition_data = []
            
            # å„ã‚·ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            for seed_dir in condition_dir.glob("seed_*"):
                if not seed_dir.is_dir():
                    continue
                
                seed_name = seed_dir.name
                print(f"   ğŸ“ Loading {condition}/{seed_name}...")
                
                # ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                summary_files = list(seed_dir.glob("summary_*.json"))
                if not summary_files:
                    print(f"      âš ï¸  No summary file found in {seed_dir}")
                    continue
                
                # ã‚¹ãƒ†ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                steps_files = list(seed_dir.glob("steps_*.csv"))
                if not steps_files:
                    print(f"      âš ï¸  No steps file found in {seed_dir}")
                    continue
                
                try:
                    # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                    with summary_files[0].open('r') as f:
                        summary = json.load(f)
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                    steps_df = pd.read_csv(steps_files[0])
                    
                    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
                    seed_data = {
                        "seed": seed_name,
                        "summary": summary,
                        "steps": steps_df,
                        "config_hash": summary.get("config_hash", "unknown")
                    }
                    
                    condition_data.append(seed_data)
                    print(f"      âœ… Loaded {len(steps_df)} steps, {len(summary.get('episode_scores', []))} episodes")
                    
                except Exception as e:
                    print(f"      âŒ Failed to load {seed_dir}: {e}")
            
            self.data[condition] = condition_data
            print(f"   ğŸ“Š {condition}: {len(condition_data)} seeds loaded")
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        total_seeds = sum(len(data) for data in self.data.values())
        if total_seeds == 0:
            print("âŒ No valid data found!")
            return False
        
        print(f"âœ… Successfully loaded data from {total_seeds} seed experiments")
        return True
    
    def calculate_summary_statistics(self):
        """å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„çµ±è¨ˆã‚’è¨ˆç®—"""
        print("\nğŸ“ˆ Calculating summary statistics...")
        
        for condition, condition_data in self.data.items():
            if not condition_data:
                continue
            
            print(f"   ğŸ“Š Analyzing {condition}...")
            
            # å„ã‚·ãƒ¼ãƒ‰ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’åé›†
            mean_scores = []
            episode_counts = []
            total_steps = []
            llm_usage = []
            
            for seed_data in condition_data:
                summary = seed_data["summary"]
                steps_df = seed_data["steps"]
                
                mean_scores.append(summary.get("mean_score", 0))
                episode_counts.append(len(summary.get("episode_scores", [])))
                total_steps.append(len(steps_df))
                
                # LLMä½¿ç”¨ç‡è¨ˆç®—ï¼ˆè©²å½“æ¡ä»¶ã®ã¿ï¼‰
                if "llm_used" in steps_df.columns:
                    llm_usage.append(steps_df["llm_used"].mean())
            
            # çµ±è¨ˆè¨ˆç®—
            stats_dict = {
                "condition": condition,
                "n_seeds": len(condition_data),
                "mean_score": {
                    "mean": np.mean(mean_scores),
                    "std": np.std(mean_scores, ddof=1) if len(mean_scores) > 1 else 0,
                    "min": np.min(mean_scores),
                    "max": np.max(mean_scores),
                    "median": np.median(mean_scores),
                    "raw_values": mean_scores
                },
                "episodes_per_seed": {
                    "mean": np.mean(episode_counts),
                    "total": np.sum(episode_counts)
                },
                "steps_per_seed": {
                    "mean": np.mean(total_steps),
                    "total": np.sum(total_steps)
                }
            }
            
            # LLMä½¿ç”¨çµ±è¨ˆï¼ˆè©²å½“æ¡ä»¶ã®ã¿ï¼‰
            if llm_usage:
                stats_dict["llm_usage_rate"] = {
                    "mean": np.mean(llm_usage),
                    "std": np.std(llm_usage, ddof=1) if len(llm_usage) > 1 else 0,
                    "raw_values": llm_usage
                }
            
            self.summary_stats[condition] = stats_dict
            
            print(f"      âœ… Mean score: {stats_dict['mean_score']['mean']:.2f} Â± {stats_dict['mean_score']['std']:.2f}")
            print(f"      ğŸ“Š Total episodes: {stats_dict['episodes_per_seed']['total']}")
            print(f"      ğŸ”¢ Total steps: {stats_dict['steps_per_seed']['total']}")
    
    def perform_statistical_tests(self) -> Dict[str, Any]:
        """çµ±è¨ˆçš„æ¤œå®šã‚’å®Ÿè¡Œ"""
        print("\nğŸ§ª Performing statistical tests...")
        
        test_results = {}
        
        # æ¡ä»¶é–“æ¯”è¼ƒã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
        condition_scores = {}
        for condition, stats in self.summary_stats.items():
            if stats and "mean_score" in stats:
                condition_scores[condition] = stats["mean_score"]["raw_values"]
        
        if len(condition_scores) < 2:
            print("   âš ï¸  Insufficient conditions for statistical testing")
            return test_results
        
        # 1. æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkï¼‰
        print("   ğŸ” Testing normality (Shapiro-Wilk)...")
        normality_results = {}
        for condition, scores in condition_scores.items():
            if len(scores) >= 3:  # Shapiro-Wilkã«ã¯æœ€ä½3ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
                stat, p_value = scipy_stats.shapiro(scores)
                normality_results[condition] = {
                    "statistic": stat,
                    "p_value": p_value,
                    "is_normal": p_value > 0.05
                }
                print(f"      {condition}: W={stat:.4f}, p={p_value:.4f} ({'Normal' if p_value > 0.05 else 'Non-normal'})")
        
        test_results["normality"] = normality_results
        
        # 2. ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneï¼‰
        if len(condition_scores) >= 2:
            print("   âš–ï¸  Testing equal variances (Levene)...")
            score_groups = [scores for scores in condition_scores.values() if len(scores) > 0]
            if len(score_groups) >= 2:
                stat, p_value = scipy_stats.levene(*score_groups)
                test_results["equal_variances"] = {
                    "statistic": stat,
                    "p_value": p_value,
                    "equal_variances": p_value > 0.05
                }
                print(f"      Levene: W={stat:.4f}, p={p_value:.4f} ({'Equal' if p_value > 0.05 else 'Unequal'} variances)")
        
        # 3. ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰ã¾ãŸã¯ Kruskal-Wallisæ¤œå®š
        if len(condition_scores) >= 2:
            score_groups = [scores for scores in condition_scores.values() if len(scores) > 0]
            condition_names = [name for name, scores in condition_scores.items() if len(scores) > 0]
            
            # æ­£è¦æ€§ã¨ç­‰åˆ†æ•£æ€§ã«åŸºã¥ã„ã¦æ¤œå®šé¸æŠ
            all_normal = all(normality_results.get(cond, {}).get("is_normal", False) for cond in condition_names)
            equal_vars = test_results.get("equal_variances", {}).get("equal_variances", False)
            
            if all_normal and equal_vars:
                print("   ğŸ“Š Performing ANOVA (parametric)...")
                stat, p_value = scipy_stats.f_oneway(*score_groups)
                test_name = "ANOVA"
            else:
                print("   ğŸ“Š Performing Kruskal-Wallis (non-parametric)...")
                stat, p_value = scipy_stats.kruskal(*score_groups)
                test_name = "Kruskal-Wallis"
            
            test_results["overall_comparison"] = {
                "test_name": test_name,
                "statistic": stat,
                "p_value": p_value,
                "significant": p_value < 0.05,
                "conditions": condition_names
            }
            
            print(f"      {test_name}: stat={stat:.4f}, p={p_value:.4f} ({'Significant' if p_value < 0.05 else 'Not significant'})")
        
        # 4. äº‹å¾Œæ¤œå®šï¼ˆãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼‰
        if test_results.get("overall_comparison", {}).get("significant", False):
            print("   ğŸ” Performing pairwise comparisons...")
            pairwise_results = {}
            
            condition_list = list(condition_scores.keys())
            for i in range(len(condition_list)):
                for j in range(i+1, len(condition_list)):
                    cond1, cond2 = condition_list[i], condition_list[j]
                    scores1, scores2 = condition_scores[cond1], condition_scores[cond2]
                    
                    # Mann-Whitney Uæ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰
                    stat, p_value = scipy_stats.mannwhitneyu(scores1, scores2, alternative='two-sided')
                    
                    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰
                    pooled_std = np.sqrt(((len(scores1)-1)*np.var(scores1, ddof=1) + (len(scores2)-1)*np.var(scores2, ddof=1)) / (len(scores1)+len(scores2)-2))
                    cohens_d = (np.mean(scores1) - np.mean(scores2)) / pooled_std if pooled_std > 0 else 0
                    
                    pair_key = f"{cond1}_vs_{cond2}"
                    pairwise_results[pair_key] = {
                        "statistic": stat,
                        "p_value": p_value,
                        "significant": p_value < 0.05,
                        "cohens_d": cohens_d,
                        "effect_size": "small" if abs(cohens_d) < 0.5 else "medium" if abs(cohens_d) < 0.8 else "large"
                    }
                    
                    print(f"      {cond1} vs {cond2}: U={stat:.2f}, p={p_value:.4f}, d={cohens_d:.3f} ({'*' if p_value < 0.05 else ''})")
            
            test_results["pairwise_comparisons"] = pairwise_results
        
        return test_results
    
    def create_visualizations(self, output_dir: str = None):
        """å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚’ä½œæˆ"""
        if output_dir is None:
            output_dir = self.base_dir
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“Š Creating visualizations...")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        plot_data = []
        for condition, stats in self.summary_stats.items():
            if stats and "mean_score" in stats:
                for score in stats["mean_score"]["raw_values"]:
                    plot_data.append({"Condition": condition, "Mean_Score": score})
        
        if not plot_data:
            print("   âš ï¸  No data available for visualization")
            return
        
        df = pd.DataFrame(plot_data)
        
        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use('default')
        sns.set_palette("husl")
        
        # å›³1: ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Tower Defense Experiment Results (Real Data Only)', fontsize=16, fontweight='bold')
        
        # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        sns.boxplot(data=df, x="Condition", y="Mean_Score", ax=axes[0, 0])
        axes[0, 0].set_title('Score Distribution by Condition')
        axes[0, 0].set_ylabel('Mean Score per Seed')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
        sns.violinplot(data=df, x="Condition", y="Mean_Score", ax=axes[0, 1])
        axes[0, 1].set_title('Score Distribution (Violin Plot)')
        axes[0, 1].set_ylabel('Mean Score per Seed')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # å¹³å‡å€¤ã¨ä¿¡é ¼åŒºé–“
        means = df.groupby('Condition')['Mean_Score'].agg(['mean', 'std', 'count']).reset_index()
        means['se'] = means['std'] / np.sqrt(means['count'])
        means['ci'] = means['se'] * 1.96  # 95% CI
        
        axes[1, 0].bar(means['Condition'], means['mean'], yerr=means['ci'], capsize=5, alpha=0.7)
        axes[1, 0].set_title('Mean Scores with 95% Confidence Intervals')
        axes[1, 0].set_ylabel('Mean Score')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # æ¡ä»¶åˆ¥è©³ç´°çµ±è¨ˆ
        stats_text = "Summary Statistics:\\n\\n"
        for condition, stats in self.summary_stats.items():
            if stats and "mean_score" in stats:
                ms = stats["mean_score"]
                stats_text += f"{condition}:\\n"
                stats_text += f"  Mean: {ms['mean']:.2f}\\n"
                stats_text += f"  Std: {ms['std']:.2f}\\n"
                stats_text += f"  N: {stats['n_seeds']}\\n\\n"
        
        axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, 
                       fontsize=10, verticalalignment='top', fontfamily='monospace')
        axes[1, 1].set_title('Statistical Summary')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜
        viz_file = output_path / "real_data_analysis.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   âœ… Main visualization saved: {viz_file}")
        
        # å›³2: è©³ç´°åˆ†æï¼ˆLLMä½¿ç”¨ç‡ãªã©ï¼‰
        if any("llm_usage_rate" in stats for stats in self.summary_stats.values()):
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            
            llm_conditions = []
            llm_usage_rates = []
            
            for condition, stats in self.summary_stats.items():
                if "llm_usage_rate" in stats:
                    llm_conditions.append(condition)
                    llm_usage_rates.append(stats["llm_usage_rate"]["mean"])
            
            if llm_conditions:
                ax.bar(llm_conditions, llm_usage_rates, alpha=0.7)
                ax.set_title('LLM Usage Rate by Condition')
                ax.set_ylabel('LLM Usage Rate')
                ax.set_ylim(0, 1)
                
                for i, rate in enumerate(llm_usage_rates):
                    ax.text(i, rate + 0.02, f'{rate:.2%}', ha='center')
                
                plt.tight_layout()
                
                llm_viz_file = output_path / "llm_usage_analysis.png"
                plt.savefig(llm_viz_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"   âœ… LLM analysis visualization saved: {llm_viz_file}")
    
    def generate_analysis_report(self, statistical_tests: Dict[str, Any], output_file: str = None):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if output_file is None:
            output_file = self.base_dir / "real_data_analysis_report.md"
        
        print(f"\nğŸ“ Generating analysis report...")
        
        report = f"""# Tower Defense å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## åˆ†ææ¦‚è¦

- **åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: å®Ÿæ¸¬å®Ÿé¨“ãƒ­ã‚°ã®ã¿ï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰
- **åˆ†æå¯¾è±¡æ¡ä»¶**: {len(self.summary_stats)}æ¡ä»¶
- **çµ±è¨ˆæ‰‹æ³•**: ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šä¸­å¿ƒ

## ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨** - åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ä¸€åˆ‡ãªã—  
âœ… **å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“** - å®Œå…¨ãªå†ç¾å¯èƒ½æ€§  
âœ… **è¨­å®šãƒãƒƒã‚·ãƒ¥ç®¡ç†** - å®Ÿé¨“æ¡ä»¶ã®å³å¯†ãªè¿½è·¡  
âœ… **ãƒ­ã‚°æ•´åˆæ€§æ¤œè¨¼** - ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ç¢ºä¿  

## è¨˜è¿°çµ±è¨ˆ

"""
        
        # æ¡ä»¶åˆ¥çµ±è¨ˆ
        for condition, stats in self.summary_stats.items():
            if not stats:
                continue
            
            ms = stats["mean_score"]
            report += f"""### {condition.upper()}

- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: {stats['n_seeds']}ã‚·ãƒ¼ãƒ‰
- **ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {stats['episodes_per_seed']['total']}
- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: {stats['steps_per_seed']['total']}
- **å¹³å‡ã‚¹ã‚³ã‚¢**: {ms['mean']:.2f} Â± {ms['std']:.2f}
- **ã‚¹ã‚³ã‚¢ç¯„å›²**: {ms['min']:.2f} - {ms['max']:.2f}
- **ä¸­å¤®å€¤**: {ms['median']:.2f}

"""
            
            # LLMä½¿ç”¨çµ±è¨ˆï¼ˆè©²å½“æ¡ä»¶ã®ã¿ï¼‰
            if "llm_usage_rate" in stats:
                llm = stats["llm_usage_rate"]
                report += f"- **LLMä½¿ç”¨ç‡**: {llm['mean']:.2%} Â± {llm['std']:.2%}\n\n"
        
        # çµ±è¨ˆçš„æ¤œå®šçµæœ
        if statistical_tests:
            report += """## çµ±è¨ˆçš„æ¤œå®šçµæœ

### å‰ææ¡ä»¶æ¤œå®š

"""
            
            # æ­£è¦æ€§æ¤œå®š
            if "normality" in statistical_tests:
                report += "#### æ­£è¦æ€§æ¤œå®š (Shapiro-Wilk)\n\n"
                for condition, result in statistical_tests["normality"].items():
                    status = "âœ… æ­£è¦åˆ†å¸ƒ" if result["is_normal"] else "âŒ éæ­£è¦åˆ†å¸ƒ"
                    report += f"- **{condition}**: W={result['statistic']:.4f}, p={result['p_value']:.4f} ({status})\n"
                report += "\n"
            
            # ç­‰åˆ†æ•£æ€§æ¤œå®š
            if "equal_variances" in statistical_tests:
                ev = statistical_tests["equal_variances"]
                status = "âœ… ç­‰åˆ†æ•£" if ev["equal_variances"] else "âŒ ä¸ç­‰åˆ†æ•£"
                report += f"#### ç­‰åˆ†æ•£æ€§æ¤œå®š (Levene)\n\n"
                report += f"- **çµæœ**: W={ev['statistic']:.4f}, p={ev['p_value']:.4f} ({status})\n\n"
            
            # å…¨ä½“æ¯”è¼ƒ
            if "overall_comparison" in statistical_tests:
                oc = statistical_tests["overall_comparison"]
                status = "âœ… æœ‰æ„å·®ã‚ã‚Š" if oc["significant"] else "âŒ æœ‰æ„å·®ãªã—"
                report += f"### å…¨ä½“æ¯”è¼ƒ ({oc['test_name']})\n\n"
                report += f"- **çµ±è¨ˆé‡**: {oc['statistic']:.4f}\n"
                report += f"- **på€¤**: {oc['p_value']:.4f}\n"
                report += f"- **çµæœ**: {status} (Î±=0.05)\n\n"
            
            # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ
            if "pairwise_comparisons" in statistical_tests:
                report += "### ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ (Mann-Whitney Uæ¤œå®š)\n\n"
                report += "| æ¯”è¼ƒ | Uçµ±è¨ˆé‡ | på€¤ | Cohen's d | åŠ¹æœé‡ | æœ‰æ„å·® |\n"
                report += "|------|---------|-----|-----------|--------|--------|\n"
                
                for pair, result in statistical_tests["pairwise_comparisons"].items():
                    sig_mark = "âœ…" if result["significant"] else "âŒ"
                    report += f"| {pair.replace('_vs_', ' vs ')} | {result['statistic']:.2f} | {result['p_value']:.4f} | {result['cohens_d']:.3f} | {result['effect_size']} | {sig_mark} |\n"
                
                report += "\n"
        
        # çµè«–
        report += """## çµè«–

### ä¸»è¦ãªç™ºè¦‹

"""
        
        # æœ€é«˜æ€§èƒ½æ¡ä»¶ã®ç‰¹å®š
        best_condition = max(self.summary_stats.items(), 
                           key=lambda x: x[1].get("mean_score", {}).get("mean", 0) if x[1] else 0)
        
        if best_condition[1]:
            best_score = best_condition[1]["mean_score"]["mean"]
            report += f"1. **æœ€é«˜æ€§èƒ½**: {best_condition[0]} (å¹³å‡ã‚¹ã‚³ã‚¢: {best_score:.2f})\n"
        
        # çµ±è¨ˆçš„æœ‰æ„å·®
        if statistical_tests.get("overall_comparison", {}).get("significant", False):
            report += "2. **çµ±è¨ˆçš„æœ‰æ„å·®**: æ¡ä»¶é–“ã«æœ‰æ„ãªæ€§èƒ½å·®ãŒç¢ºèªã•ã‚ŒãŸ\n"
        else:
            report += "2. **çµ±è¨ˆçš„æœ‰æ„å·®**: æ¡ä»¶é–“ã«æœ‰æ„ãªæ€§èƒ½å·®ã¯ç¢ºèªã•ã‚Œãªã‹ã£ãŸ\n"
        
        # LLMåŠ¹æœ
        llm_conditions = [cond for cond, stats in self.summary_stats.items() 
                         if stats and "llm_usage_rate" in stats]
        if llm_conditions:
            report += f"3. **LLMåŠ¹æœ**: {len(llm_conditions)}æ¡ä»¶ã§LLMä»‹å…¥ã‚’ç¢ºèª\n"
        
        report += """
### ç ”ç©¶ã®ä¿¡é ¼æ€§

- âœ… **ãƒ‡ãƒ¼ã‚¿ã®çœŸæ­£æ€§**: å…¨ã¦å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã€åˆæˆãƒ‡ãƒ¼ã‚¿ãªã—
- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰ã€è¨­å®šãƒãƒƒã‚·ãƒ¥ç®¡ç†
- âœ… **çµ±è¨ˆçš„å¦¥å½“æ€§**: é©åˆ‡ãªæ¤œå®šæ‰‹æ³•ã®é¸æŠ
- âœ… **é€æ˜æ€§**: å…¨å®Ÿé¨“ãƒ­ã‚°ã®ä¿å­˜ãƒ»å…¬é–‹

### ä»Šå¾Œã®èª²é¡Œ

1. **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ‹¡å¤§**: ã‚ˆã‚Šå¤šãã®ã‚·ãƒ¼ãƒ‰ã§ã®å®Ÿé¨“
2. **é•·æœŸå®Ÿé¨“**: ã‚ˆã‚Šå¤šãã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§ã®æ€§èƒ½è©•ä¾¡
3. **ç’°å¢ƒå¤šæ§˜æ€§**: ç•°ãªã‚‹ã‚²ãƒ¼ãƒ è¨­å®šã§ã®æ¤œè¨¼
4. **LLMæœ€é©åŒ–**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«åŸºã¥ã„ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
*çµ±è¨ˆåˆ†æã«ã¯scipy.statsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚*
"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"   âœ… Analysis report saved: {output_file}")
        
        return report
    
    def run_complete_analysis(self, output_dir: str = None):
        """å®Œå…¨åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Starting complete real data analysis...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_real_data():
            return False
        
        # çµ±è¨ˆè¨ˆç®—
        self.calculate_summary_statistics()
        
        # çµ±è¨ˆæ¤œå®š
        statistical_tests = self.perform_statistical_tests()
        
        # å¯è¦–åŒ–
        self.create_visualizations(output_dir)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_analysis_report(statistical_tests, 
                                    Path(output_dir or self.base_dir) / "real_data_analysis_report.md")
        
        print("\nâœ… Complete real data analysis finished!")
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Analyze real experimental data (no synthetic data)")
    parser.add_argument("data_dir", help="Directory containing experimental data")
    parser.add_argument("--output", help="Output directory for analysis results")
    
    args = parser.parse_args()
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = RealDataAnalyzer(args.data_dir)
    success = analyzer.run_complete_analysis(args.output)
    
    if not success:
        exit(1)


if __name__ == "__main__":
    main()
