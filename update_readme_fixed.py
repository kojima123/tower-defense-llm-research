#!/usr/bin/env python3
"""
å®Ÿæ¸¬çµæœã‹ã‚‰ã®è‡ªå‹•READMEæ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 
åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡ä½¿ç”¨ã›ãšã€å®Ÿéš›ã®å®Ÿé¨“çµæœã®ã¿ã‹ã‚‰READMEã‚’ç”Ÿæˆãƒ»æ›´æ–°
"""
import json
import pandas as pd
from pathlib import Path
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
import re


class ReadmeUpdater:
    """å®Ÿæ¸¬çµæœãƒ™ãƒ¼ã‚¹READMEæ›´æ–°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_dir: str):
        self.project_dir = Path(project_dir)
        self.readme_path = self.project_dir / "README.md"
        self.results_data = {}
        
    def load_experiment_results(self, results_dir: str) -> bool:
        """å®Ÿé¨“çµæœã‚’èª­ã¿è¾¼ã¿"""
        results_path = Path(results_dir)
        
        print("ğŸ“Š Loading experiment results for README update...")
        
        # å®Ÿé¨“ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        summary_files = list(results_path.glob("**/experiment_summary*.json"))
        if not summary_files:
            print("   âš ï¸  No experiment summary found")
            return False
        
        # æœ€æ–°ã®ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        latest_summary = max(summary_files, key=lambda x: x.stat().st_mtime)
        print(f"   ğŸ“ Loading: {latest_summary}")
        
        try:
            with latest_summary.open('r') as f:
                self.results_data = json.load(f)
            
            print(f"   âœ… Loaded results for {len(self.results_data.get('results', {}))} conditions")
            return True
            
        except Exception as e:
            print(f"   âŒ Failed to load results: {e}")
            return False
    
    def extract_performance_summary(self) -> Dict[str, Any]:
        """å®Ÿé¨“çµæœã‹ã‚‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’æŠ½å‡º"""
        if not self.results_data or "results" not in self.results_data:
            return {}
        
        summary = {
            "experiment_date": datetime.now().strftime("%Y-%m-%d"),
            "total_conditions": len(self.results_data["results"]),
            "seeds_used": self.results_data.get("seeds", []),
            "episodes_per_condition": self.results_data.get("total_episodes_per_condition", 0),
            "conditions": {}
        }
        
        # æ¡ä»¶åˆ¥çµæœã®æŠ½å‡º
        for condition, data in self.results_data["results"].items():
            if "results" in data and data["results"]:
                scores = [r["mean_score"] for r in data["results"]]
                
                condition_summary = {
                    "mean_score": sum(scores) / len(scores),
                    "std_score": (sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores))**0.5,
                    "min_score": min(scores),
                    "max_score": max(scores),
                    "sample_size": len(scores),
                    "total_time": data.get("total_time", 0)
                }
                
                summary["conditions"][condition] = condition_summary
        
        # æœ€é«˜æ€§èƒ½æ¡ä»¶ã®ç‰¹å®š
        if summary["conditions"]:
            best_condition = max(summary["conditions"].items(), 
                               key=lambda x: x[1]["mean_score"])
            summary["best_condition"] = {
                "name": best_condition[0],
                "score": best_condition[1]["mean_score"]
            }
        
        return summary
    
    def generate_results_section(self, performance_summary: Dict[str, Any]) -> str:
        """å®Ÿé¨“çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        if not performance_summary:
            return "## å®Ÿé¨“çµæœ\\n\\n*å®Ÿé¨“çµæœã¯ã¾ã åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚*\\n"
        
        section = f"""## å®Ÿé¨“çµæœ

### æœ€æ–°å®Ÿé¨“ ({performance_summary['experiment_date']})

**å®Ÿé¨“è¨­å®š:**
- æ¡ä»¶æ•°: {performance_summary['total_conditions']}
- ä½¿ç”¨ã‚·ãƒ¼ãƒ‰: {performance_summary['seeds_used']}
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°/æ¡ä»¶: {performance_summary['episodes_per_condition']}
- ãƒ‡ãƒ¼ã‚¿å“è³ª: âœ… å®Ÿæ¸¬ã®ã¿ï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ¡ä»¶ | å¹³å‡ã‚¹ã‚³ã‚¢ | æ¨™æº–åå·® | æœ€å°-æœ€å¤§ | ã‚µãƒ³ãƒ—ãƒ«æ•° |
|------|------------|----------|-----------|------------|
"""
        
        # æ¡ä»¶åˆ¥çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        for condition, stats in performance_summary["conditions"].items():
            section += f"| {condition} | {stats['mean_score']:.2f} | {stats['std_score']:.2f} | {stats['min_score']:.0f}-{stats['max_score']:.0f} | {stats['sample_size']} |\n"
        
        # æœ€é«˜æ€§èƒ½ã®å¼·èª¿
        if "best_condition" in performance_summary:
            best = performance_summary["best_condition"]
            section += f"\n**ğŸ† æœ€é«˜æ€§èƒ½**: {best['name']} (å¹³å‡ã‚¹ã‚³ã‚¢: {best['score']:.2f})\n"
        
        section += "\n### ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼\n\n"
        section += "- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ä¸€åˆ‡ãªã—\n"
        section += "- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“\n"
        section += "- âœ… **é€æ˜æ€§**: å…¨å®Ÿé¨“ãƒ­ã‚°å…¬é–‹\n"
        section += "- âœ… **çµ±è¨ˆçš„å¦¥å½“æ€§**: é©åˆ‡ãªæ¤œå®šæ‰‹æ³•ä½¿ç”¨\n\n"
        
        return section
    
    def create_readme_content(self, performance_summary: Dict[str, Any]) -> str:
        """å®Œå…¨ãªREADMEå†…å®¹ã‚’ä½œæˆ"""
        results_section = self.generate_results_section(performance_summary)
        
        return f"""# Tower Defense ELM+LLM Research Project

**ç§‘å­¦çš„å³å¯†æ€§ã‚’é‡è¦–ã—ãŸã‚¿ãƒ¯ãƒ¼ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ **

[![Data Quality](https://img.shields.io/badge/Data-Real%20Only-green)]()
[![Reproducibility](https://img.shields.io/badge/Reproducibility-Fixed%20Seeds-blue)]()
[![Transparency](https://img.shields.io/badge/Transparency-Full%20Logs-orange)]()

## æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ELMï¼ˆExtreme Learning Machineï¼‰ã¨LLMï¼ˆLarge Language Modelï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¿ãƒ¯ãƒ¼ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚**å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**ã‚’ä½¿ç”¨ã—ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ç§‘å­¦çš„ã«å³å¯†ãªå®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¦ã„ã¾ã™ã€‚

### ä¸»è¦ç‰¹å¾´

- ğŸ”¬ **ç§‘å­¦çš„å³å¯†æ€§**: å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã€åˆæˆãƒ‡ãƒ¼ã‚¿å®Œå…¨æ’é™¤
- ğŸ”„ **å®Œå…¨å†ç¾å¯èƒ½**: å›ºå®šã‚·ãƒ¼ãƒ‰ã€è¨­å®šãƒãƒƒã‚·ãƒ¥ç®¡ç†
- ğŸ“Š **çµ±è¨ˆçš„å¦¥å½“æ€§**: é©åˆ‡ãªæ¤œå®šæ‰‹æ³•ã«ã‚ˆã‚‹åˆ†æ
- ğŸ¤– **LLMçµ±åˆ**: OpenAI GPT-4o-mini ã«ã‚ˆã‚‹æˆ¦ç•¥æŒ‡å°
- ğŸ“ **é€æ˜æ€§**: å…¨å®Ÿé¨“ãƒ­ã‚°ã®å…¬é–‹ãƒ»æ¤œè¨¼å¯èƒ½

{results_section}

## æŠ€è¡“è©³ç´°

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

- **ELM (Extreme Learning Machine)**: é«˜é€Ÿå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **LLM Teacher**: OpenAI GPT-4o-mini ã«ã‚ˆã‚‹æˆ¦ç•¥æŒ‡å°
- **ç’°å¢ƒ**: Tower Defense ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼
- **ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ **: CSV + JSON + JSONL å½¢å¼

### å®Ÿé¨“æ¡ä»¶

1. **ELMå˜ä½“**: ELMã®ã¿ã§ã®å­¦ç¿’
2. **ãƒ«ãƒ¼ãƒ«æ•™å¸«**: äº‹å‰å®šç¾©ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹æŒ‡å°
3. **ãƒ©ãƒ³ãƒ€ãƒ æ•™å¸«**: ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•æŒ‡å°
4. **ELM+LLM**: LLMã«ã‚ˆã‚‹æˆ¦ç•¥çš„æŒ‡å°

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```
å®Ÿé¨“å®Ÿè¡Œ â†’ ãƒ­ã‚°è¨˜éŒ² â†’ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ â†’ çµ±è¨ˆåˆ†æ â†’ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
```

- **åˆæˆãƒ‡ãƒ¼ã‚¿æ¤œå‡º**: è‡ªå‹•çš„ã«æ’é™¤
- **è¨­å®šãƒãƒƒã‚·ãƒ¥**: å®Ÿé¨“æ¡ä»¶ã®å³å¯†ç®¡ç†
- **çµ±è¨ˆæ¤œå®š**: scipy.stats ã«ã‚ˆã‚‹ç§‘å­¦çš„åˆ†æ

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# 4æ¡ä»¶æ¯”è¼ƒå®Ÿé¨“ï¼ˆæ¨å¥¨ï¼‰
python run_fixed_seed_experiments.py --episodes 20

# å˜ä¸€æ¡ä»¶å®Ÿé¨“
python run_elm_real.py --condition elm_only --episodes 10 --seed 42

# LLMå®Ÿé¨“ï¼ˆOpenAI APIã‚­ãƒ¼å¿…è¦ï¼‰
export OPENAI_API_KEY="your-api-key"
python run_elm_llm_real.py --episodes 10 --seed 42
```

### åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```bash
# å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åˆ†æ
python analyze_real_data.py runs/real/experiment_name/

# LLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³åˆ†æ
python analyze_llm_interactions.py runs/real/experiment_name/elm_llm/seed_42/

# å®Œå…¨å®Ÿé¨“ï¼ˆå®Ÿè¡Œ+åˆ†æï¼‰
python run_complete_experiment.py --episodes 20
```

### çµæœã®ç¢ºèª

- **å®Ÿé¨“ãƒ­ã‚°**: `runs/real/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- **åˆ†æãƒ¬ãƒãƒ¼ãƒˆ**: `*_analysis_report.md`
- **å¯è¦–åŒ–**: `*.png` ãƒ•ã‚¡ã‚¤ãƒ«
- **çµ±è¨ˆçµæœ**: JSONå½¢å¼ã®ã‚µãƒãƒªãƒ¼

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
â”œâ”€â”€ run_fixed_seed_experiments.py  # 4æ¡ä»¶æ¯”è¼ƒå®Ÿé¨“
â”œâ”€â”€ run_elm_real.py                # ELMå˜ä½“å®Ÿé¨“
â”œâ”€â”€ run_elm_llm_real.py            # ELM+LLMå®Ÿé¨“
â”œâ”€â”€ analyze_real_data.py           # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åˆ†æ
â”œâ”€â”€ logger.py                      # å®Ÿæ¸¬å°‚ç”¨ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tower_defense_environment.py
â”‚   â”œâ”€â”€ elm_tower_defense_agent.py
â”‚   â””â”€â”€ llm_teacher.py
â””â”€â”€ runs/real/                     # å®Ÿæ¸¬å®Ÿé¨“ãƒ­ã‚°
```

## ç ”ç©¶ã®ä¿¡é ¼æ€§

### ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: `validate_no_synthetic_data()` ã«ã‚ˆã‚‹æ¤œè¨¼
- âœ… **å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“**: å®Œå…¨ãªå†ç¾å¯èƒ½æ€§
- âœ… **è¨­å®šãƒãƒƒã‚·ãƒ¥**: å®Ÿé¨“æ¡ä»¶ã®å³å¯†ãªè¿½è·¡
- âœ… **ãƒ­ã‚°æ•´åˆæ€§**: è‡ªå‹•æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

### çµ±è¨ˆåˆ†æ

- ğŸ“Š **è¨˜è¿°çµ±è¨ˆ**: å¹³å‡ã€æ¨™æº–åå·®ã€ç¯„å›²ã€ä¸­å¤®å€¤
- ğŸ§ª **æ¤œå®š**: Shapiro-Wilkã€Leveneã€ANOVA/Kruskal-Wallis
- ğŸ“ˆ **åŠ¹æœé‡**: Cohen's d ã«ã‚ˆã‚‹å®Ÿç”¨çš„æœ‰æ„æ€§
- ğŸ¯ **å¤šé‡æ¯”è¼ƒ**: Mann-Whitney Uæ¤œå®š

## è²¢çŒ®

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

**æ³¨æ„**: å…¨ã¦ã®è²¢çŒ®ã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’å«ã¾ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License - è©³ç´°ã¯ [LICENSE](LICENSE) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§

## å¼•ç”¨

```bibtex
@software{{tower_defense_elm_llm,
  title={{Tower Defense ELM+LLM Research Project}},
  author={{Research Team}},
  year={{2025}},
  url={{https://github.com/your-repo/tower-defense-llm}}
}}
```

---

*ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€ç§‘å­¦çš„å³å¯†æ€§ã‚’æœ€å„ªå…ˆã«é–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚*
"""
    
    def update_readme(self, results_dir: str) -> bool:
        """READMEã‚’å®Ÿæ¸¬çµæœã§æ›´æ–°"""
        print("ğŸ“ Updating README with real experimental results...")
        
        # å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿
        if not self.load_experiment_results(results_dir):
            return False
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼æŠ½å‡º
        performance_summary = self.extract_performance_summary()
        
        # READMEå†…å®¹ç”Ÿæˆ
        readme_content = self.create_readme_content(performance_summary)
        
        # READMEä¿å­˜
        try:
            with self.readme_path.open('w', encoding='utf-8') as f:
                f.write(readme_content)
            
            print(f"   âœ… README updated: {self.readme_path}")
            print(f"   ğŸ“Š Included {len(performance_summary.get('conditions', {}))} conditions")
            
            if "best_condition" in performance_summary:
                best = performance_summary["best_condition"]
                print(f"   ğŸ† Best performance: {best['name']} ({best['score']:.2f})")
            
            return True
            
        except Exception as e:
            print(f"   âŒ Failed to write README: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Update README with real experimental results")
    parser.add_argument("results_dir", help="Directory containing experiment results")
    parser.add_argument("--project_dir", default=".", help="Project root directory")
    
    args = parser.parse_args()
    
    # READMEæ›´æ–°å®Ÿè¡Œ
    updater = ReadmeUpdater(args.project_dir)
    success = updater.update_readme(args.results_dir)
    
    if success:
        print("\nâœ… README successfully updated with real experimental results!")
    else:
        print("\nâŒ Failed to update README")
        exit(1)


if __name__ == "__main__":
    main()
