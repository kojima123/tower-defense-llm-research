#!/usr/bin/env python3
"""
4æ¡ä»¶çµ±ä¸€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ 
åŒä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ãƒ»åŒä¸€seedé›†åˆã§ã®å³å¯†æ¯”è¼ƒ
"""
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import subprocess
import tempfile


class UnifiedAblationExperiment:
    """4æ¡ä»¶çµ±ä¸€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_dir: str = "."):
        self.project_dir = Path(project_dir)
        self.conditions = ["elm_only", "rule_teacher", "random_teacher", "elm_llm"]
        
    def run_unified_experiment(self, episodes: int = 10, seeds: List[int] = None, experiment_name: str = None) -> str:
        """çµ±ä¸€æ¡ä»¶ã§ã®4æ¡ä»¶å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        if seeds is None:
            seeds = [42, 123, 456]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒ¼ãƒ‰
        
        if experiment_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"unified_ablation_{timestamp}"
        
        print(f"ğŸš€ Starting unified ablation experiment: {experiment_name}")
        print(f"ğŸ“Š Conditions: {self.conditions}")
        print(f"ğŸ² Seeds: {seeds}")
        print(f"ğŸ“ˆ Episodes per condition per seed: {episodes}")
        
        experiment_dir = self.project_dir / "runs" / "real" / experiment_name
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“è¨­å®šä¿å­˜
        config = {
            "experiment_name": experiment_name,
            "conditions": self.conditions,
            "seeds": seeds,
            "episodes_per_seed": episodes,
            "total_runs": len(self.conditions) * len(seeds),
            "timestamp": datetime.now().isoformat()
        }
        
        config_path = experiment_dir / "experiment_config.json"
        with config_path.open('w') as f:
            json.dump(config, f, indent=2)
        
        print(f"ğŸ“ Experiment directory: {experiment_dir}")
        print(f"âš™ï¸  Configuration saved: {config_path}")
        
        # å„æ¡ä»¶ã§å®Ÿé¨“å®Ÿè¡Œ
        results = {}
        
        for condition in self.conditions:
            print(f"\nğŸ”¬ Running condition: {condition}")
            condition_results = []
            
            for seed in seeds:
                print(f"  ğŸ² Seed {seed}...")
                
                try:
                    # å®Ÿé¨“å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
                    cmd = [
                        "python", "run_experiment_cli_fixed.py", "run",
                        "--teachers", condition,
                        "--seeds", str(seed),
                        "--episodes", str(episodes),
                        "--experiment-name", f"{experiment_name}_{condition}_seed{seed}"
                    ]
                    
                    # å®Ÿé¨“å®Ÿè¡Œ
                    result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_dir)
                    
                    if result.returncode == 0:
                        print(f"    âœ… Success")
                        condition_results.append({
                            "seed": seed,
                            "status": "success",
                            "episodes": episodes
                        })
                    else:
                        print(f"    âŒ Failed: {result.stderr}")
                        condition_results.append({
                            "seed": seed,
                            "status": "failed",
                            "error": result.stderr
                        })
                
                except Exception as e:
                    print(f"    âŒ Exception: {e}")
                    condition_results.append({
                        "seed": seed,
                        "status": "exception",
                        "error": str(e)
                    })
            
            results[condition] = condition_results
        
        # çµæœä¿å­˜
        results_path = experiment_dir / "experiment_results.json"
        with results_path.open('w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ… Unified ablation experiment completed")
        print(f"ğŸ“Š Results saved: {results_path}")
        
        return str(experiment_dir)
    
    def analyze_unified_results(self, experiment_dir: str) -> Dict[str, Any]:
        """çµ±ä¸€å®Ÿé¨“çµæœã‚’åˆ†æ"""
        experiment_path = Path(experiment_dir)
        
        print(f"ğŸ“Š Analyzing unified experiment results: {experiment_path.name}")
        
        # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åé›†
        analysis = {
            "experiment_name": experiment_path.name,
            "conditions": {},
            "statistical_tests": {},
            "summary": {}
        }
        
        # å„æ¡ä»¶ã®ãƒ‡ãƒ¼ã‚¿åé›†
        for condition in self.conditions:
            condition_data = {
                "final_scores": [],
                "episode_counts": [],
                "seeds": [],
                "data_files": []
            }
            
            # æ¡ä»¶åˆ¥CSVãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            csv_files = list(experiment_path.glob(f"**/*{condition}*/*.csv"))
            csv_files.extend(list(self.project_dir.glob(f"runs/real/**/*{condition}*.csv")))
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    if len(df) == 0:
                        continue
                    
                    # æ¡ä»¶ç¢ºèª
                    if 'condition' in df.columns and df['condition'].iloc[0] == condition:
                        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ¥æœ€çµ‚ã‚¹ã‚³ã‚¢
                        if 'episode' in df.columns and 'score' in df.columns:
                            episode_scores = df.groupby('episode')['score'].last().tolist()
                            condition_data["final_scores"].extend(episode_scores)
                            condition_data["episode_counts"].append(len(episode_scores))
                        
                        if 'seed' in df.columns:
                            condition_data["seeds"].append(df['seed'].iloc[0])
                        
                        condition_data["data_files"].append(str(csv_file.relative_to(self.project_dir)))
                
                except Exception as e:
                    print(f"âš ï¸  Warning: Could not process {csv_file}: {e}")
                    continue
            
            analysis["conditions"][condition] = condition_data
        
        # çµ±è¨ˆåˆ†æ
        analysis["statistical_tests"] = self.perform_unified_statistical_tests(analysis["conditions"])
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        analysis["summary"] = self.generate_unified_summary(analysis["conditions"], analysis["statistical_tests"])
        
        return analysis
    
    def perform_unified_statistical_tests(self, conditions: Dict[str, Dict]) -> Dict[str, Any]:
        """çµ±ä¸€å®Ÿé¨“ã®çµ±è¨ˆæ¤œå®š"""
        print("ğŸ§ª Performing unified statistical tests...")
        
        tests = {
            "descriptive_stats": {},
            "normality_tests": {},
            "anova": None,
            "pairwise_comparisons": {},
            "effect_sizes": {}
        }
        
        # æ¡ä»¶åˆ¥è¨˜è¿°çµ±è¨ˆ
        condition_scores = {}
        for condition, data in conditions.items():
            if data["final_scores"]:
                scores = np.array(data["final_scores"])
                condition_scores[condition] = scores
                
                tests["descriptive_stats"][condition] = {
                    "mean": float(np.mean(scores)),
                    "std": float(np.std(scores, ddof=1) if len(scores) > 1 else 0),
                    "median": float(np.median(scores)),
                    "min": float(np.min(scores)),
                    "max": float(np.max(scores)),
                    "n": len(scores),
                    "sem": float(np.std(scores, ddof=1) / np.sqrt(len(scores)) if len(scores) > 1 else 0)
                }
        
        if len(condition_scores) < 2:
            return tests
        
        try:
            from scipy import stats as scipy_stats
            
            # æ­£è¦æ€§æ¤œå®š
            for condition, scores in condition_scores.items():
                if len(scores) >= 3:
                    stat, p_value = scipy_stats.shapiro(scores)
                    tests["normality_tests"][condition] = {
                        "statistic": float(stat),
                        "p_value": float(p_value),
                        "is_normal": p_value > 0.05
                    }
            
            # ç¾¤é–“æ¯”è¼ƒæ¤œå®š
            score_groups = list(condition_scores.values())
            if len(score_groups) >= 2:
                # æ­£è¦æ€§ã«åŸºã¥ã„ã¦æ¤œå®šé¸æŠ
                all_normal = all(
                    tests["normality_tests"].get(cond, {}).get("is_normal", False) 
                    for cond in condition_scores.keys()
                )
                
                if all_normal and len(score_groups) > 2:
                    # ANOVA
                    f_stat, p_value = scipy_stats.f_oneway(*score_groups)
                    tests["anova"] = {
                        "test": "ANOVA",
                        "statistic": float(f_stat),
                        "p_value": float(p_value),
                        "significant": p_value < 0.05
                    }
                else:
                    # Kruskal-Wallis
                    h_stat, p_value = scipy_stats.kruskal(*score_groups)
                    tests["anova"] = {
                        "test": "Kruskal-Wallis",
                        "statistic": float(h_stat),
                        "p_value": float(p_value),
                        "significant": p_value < 0.05
                    }
            
            # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ
            condition_names = list(condition_scores.keys())
            for i, cond1 in enumerate(condition_names):
                for j, cond2 in enumerate(condition_names[i+1:], i+1):
                    scores1 = condition_scores[cond1]
                    scores2 = condition_scores[cond2]
                    
                    if len(scores1) >= 2 and len(scores2) >= 2:
                        # Mann-Whitney Uæ¤œå®š
                        u_stat, p_value = scipy_stats.mannwhitneyu(scores1, scores2, alternative='two-sided')
                        
                        # Cohen's d
                        pooled_std = np.sqrt(((len(scores1) - 1) * np.var(scores1, ddof=1) + 
                                            (len(scores2) - 1) * np.var(scores2, ddof=1)) / 
                                           (len(scores1) + len(scores2) - 2))
                        cohens_d = (np.mean(scores1) - np.mean(scores2)) / pooled_std if pooled_std > 0 else 0
                        
                        pair_key = f"{cond1}_vs_{cond2}"
                        tests["pairwise_comparisons"][pair_key] = {
                            "u_statistic": float(u_stat),
                            "p_value": float(p_value),
                            "cohens_d": float(cohens_d),
                            "significant": p_value < 0.05,
                            "effect_size": "large" if abs(cohens_d) >= 0.8 else "medium" if abs(cohens_d) >= 0.5 else "small"
                        }
                        
                        tests["effect_sizes"][pair_key] = float(cohens_d)
        
        except ImportError:
            print("âš ï¸  scipy not available, skipping statistical tests")
        except Exception as e:
            print(f"âš ï¸  Statistical test error: {e}")
        
        return tests
    
    def generate_unified_summary(self, conditions: Dict, tests: Dict) -> Dict[str, Any]:
        """çµ±ä¸€å®Ÿé¨“ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        summary = {
            "best_condition": None,
            "best_score": -float('inf'),
            "worst_condition": None,
            "worst_score": float('inf'),
            "significant_differences": [],
            "recommendations": []
        }
        
        # æœ€é«˜ãƒ»æœ€ä½æ€§èƒ½æ¡ä»¶ç‰¹å®š
        for condition, data in conditions.items():
            if data["final_scores"]:
                mean_score = np.mean(data["final_scores"])
                if mean_score > summary["best_score"]:
                    summary["best_score"] = mean_score
                    summary["best_condition"] = condition
                if mean_score < summary["worst_score"]:
                    summary["worst_score"] = mean_score
                    summary["worst_condition"] = condition
        
        # æœ‰æ„å·®ã®ã‚ã‚‹æ¯”è¼ƒã‚’ç‰¹å®š
        if "pairwise_comparisons" in tests:
            for pair, result in tests["pairwise_comparisons"].items():
                if result["significant"]:
                    summary["significant_differences"].append({
                        "comparison": pair,
                        "p_value": result["p_value"],
                        "effect_size": result["effect_size"],
                        "cohens_d": result["cohens_d"]
                    })
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if summary["best_condition"]:
            summary["recommendations"].append(f"æœ€é«˜æ€§èƒ½: {summary['best_condition']} (å¹³å‡ã‚¹ã‚³ã‚¢: {summary['best_score']:.2f})")
        
        if len(summary["significant_differences"]) > 0:
            summary["recommendations"].append(f"æœ‰æ„å·®ã®ã‚ã‚‹æ¯”è¼ƒ: {len(summary['significant_differences'])}çµ„")
        else:
            summary["recommendations"].append("æ¡ä»¶é–“ã«çµ±è¨ˆçš„æœ‰æ„å·®ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return summary
    
    def create_unified_ablation_table(self, analysis: Dict, output_path: str):
        """çµ±ä¸€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ã‚’ä½œæˆ"""
        print("ğŸ“Š Creating unified ablation table...")
        
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        table_content = f"""# 4æ¡ä»¶çµ±ä¸€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“çµæœ

**å®Ÿé¨“æ—¥**: {current_date}  
**å®Ÿé¨“å**: {analysis['experiment_name']}  
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: å®Ÿæ¸¬ãƒ­ã‚°ã®ã¿ä½¿ç”¨

## ğŸ“Š æ¡ä»¶åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ¡ä»¶ | å¹³å‡ã‚¹ã‚³ã‚¢ | æ¨™æº–åå·® | æ¨™æº–èª¤å·® | 95%ä¿¡é ¼åŒºé–“ | æœ€å°-æœ€å¤§ | ã‚µãƒ³ãƒ—ãƒ«æ•° | ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ•° |
|------|------------|----------|----------|-------------|-----------|------------|------------------|"""
        
        # æ¡ä»¶åˆ¥çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        for condition in self.conditions:
            if condition in analysis["conditions"] and analysis["conditions"][condition]["final_scores"]:
                data = analysis["conditions"][condition]
                stats = analysis["statistical_tests"]["descriptive_stats"].get(condition, {})
                
                # 95%ä¿¡é ¼åŒºé–“è¨ˆç®—
                if stats.get("sem", 0) > 0:
                    ci_margin = 1.96 * stats["sem"]  # è¿‘ä¼¼çš„ãª95%CI
                    ci_lower = stats["mean"] - ci_margin
                    ci_upper = stats["mean"] + ci_margin
                    ci_text = f"[{ci_lower:.2f}, {ci_upper:.2f}]"
                else:
                    ci_text = "N/A"
                
                table_content += f"""
| {condition} | {stats.get('mean', 0):.2f} | {stats.get('std', 0):.2f} | {stats.get('sem', 0):.2f} | {ci_text} | {stats.get('min', 0):.0f}-{stats.get('max', 0):.0f} | {stats.get('n', 0)} | {len(data['data_files'])} |"""
        
        # çµ±è¨ˆæ¤œå®šçµæœ
        if analysis["statistical_tests"]["anova"]:
            anova = analysis["statistical_tests"]["anova"]
            table_content += f"""

## ğŸ§ª çµ±è¨ˆæ¤œå®šçµæœ

### ç¾¤é–“æ¯”è¼ƒ
- **æ¤œå®š**: {anova['test']}
- **çµ±è¨ˆé‡**: {anova['statistic']:.4f}
- **på€¤**: {anova['p_value']:.6f}
- **æœ‰æ„å·®**: {'ã‚ã‚Š' if anova['significant'] else 'ãªã—'} (Î±=0.05)
"""
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ
        if analysis["statistical_tests"]["pairwise_comparisons"]:
            table_content += """
### ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ (Mann-Whitney Uæ¤œå®š)

| æ¯”è¼ƒ | på€¤ | Cohen's d | åŠ¹æœé‡ | æœ‰æ„å·® |
|------|-----|-----------|--------|--------|"""
            
            for pair, result in analysis["statistical_tests"]["pairwise_comparisons"].items():
                significant = "âœ…" if result['significant'] else "âŒ"
                table_content += f"""
| {pair.replace('_vs_', ' vs ')} | {result['p_value']:.6f} | {result['cohens_d']:.3f} | {result['effect_size']} | {significant} |"""
        
        # ã‚µãƒãƒªãƒ¼
        summary = analysis["summary"]
        table_content += f"""

## ğŸ† å®Ÿé¨“ã‚µãƒãƒªãƒ¼

- **æœ€é«˜æ€§èƒ½æ¡ä»¶**: {summary['best_condition']} (å¹³å‡ã‚¹ã‚³ã‚¢: {summary['best_score']:.2f})
- **æœ€ä½æ€§èƒ½æ¡ä»¶**: {summary['worst_condition']} (å¹³å‡ã‚¹ã‚³ã‚¢: {summary['worst_score']:.2f})
- **æœ‰æ„å·®ã®ã‚ã‚‹æ¯”è¼ƒ**: {len(summary['significant_differences'])}çµ„

### æ¨å¥¨äº‹é …
"""
        
        for recommendation in summary["recommendations"]:
            table_content += f"- {recommendation}\n"
        
        table_content += f"""

## ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: åˆæˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ãªã—
- âœ… **çµ±ä¸€æ¡ä»¶**: åŒä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ãƒ»åŒä¸€ã‚·ãƒ¼ãƒ‰é›†åˆ
- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“
- âœ… **é€æ˜æ€§**: å…¨å®Ÿé¨“ãƒ­ã‚°å…¬é–‹

### ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
"""
        
        for condition in self.conditions:
            if condition in analysis["conditions"]:
                data = analysis["conditions"][condition]
                if data["data_files"]:
                    table_content += f"\n**{condition}**:\n"
                    for i, file_path in enumerate(data["data_files"][:5], 1):  # æœ€åˆã®5å€‹ã¾ã§
                        table_content += f"{i}. [`{Path(file_path).name}`]({file_path})\n"
                    if len(data["data_files"]) > 5:
                        table_content += f"... ä»–{len(data['data_files'])-5}å€‹\n"
        
        table_content += """
---

*ã“ã®è¡¨ã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(table_content)
        
        print(f"âœ… Unified ablation table saved: {output_path}")
    
    def run_and_analyze(self, episodes: int = 10, seeds: List[int] = None) -> Dict[str, Any]:
        """å®Ÿé¨“å®Ÿè¡Œã¨åˆ†æã‚’çµ±åˆå®Ÿè¡Œ"""
        print("ğŸš€ Starting unified ablation experiment and analysis...")
        print("=" * 70)
        
        # å®Ÿé¨“å®Ÿè¡Œ
        experiment_dir = self.run_unified_experiment(episodes, seeds)
        
        print("\n" + "=" * 70)
        print("ğŸ“Š Starting analysis...")
        
        # çµæœåˆ†æ
        analysis = self.analyze_unified_results(experiment_dir)
        
        # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ä½œæˆ
        table_path = Path(experiment_dir) / "unified_ablation_table.md"
        self.create_unified_ablation_table(analysis, str(table_path))
        
        print("=" * 70)
        print("âœ… Unified ablation experiment completed")
        print(f"ğŸ“ Results directory: {experiment_dir}")
        print(f"ğŸ“Š Analysis table: {table_path}")
        
        return analysis


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="4æ¡ä»¶çµ±ä¸€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“")
    parser.add_argument("--episodes", type=int, default=5, help="ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)")
    parser.add_argument("--seeds", nargs="+", type=int, default=[42, 123, 456], help="ã‚·ãƒ¼ãƒ‰é›†åˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42 123 456)")
    parser.add_argument("--analyze-only", type=str, help="æ—¢å­˜å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ†æã®ã¿å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    experiment = UnifiedAblationExperiment()
    
    if args.analyze_only:
        print(f"ğŸ“Š Analyzing existing experiment: {args.analyze_only}")
        analysis = experiment.analyze_unified_results(args.analyze_only)
        table_path = Path(args.analyze_only) / "unified_ablation_table.md"
        experiment.create_unified_ablation_table(analysis, str(table_path))
        print(f"âœ… Analysis completed: {table_path}")
    else:
        print(f"ğŸš€ Running unified ablation experiment...")
        print(f"ğŸ“ˆ Episodes: {args.episodes}, Seeds: {args.seeds}")
        
        analysis = experiment.run_and_analyze(args.episodes, args.seeds)
        
        print("\nğŸ‰ Unified ablation experiment complete!")
        print(f"ğŸ† Best condition: {analysis['summary']['best_condition']}")
        print(f"ğŸ“Š Best score: {analysis['summary']['best_score']:.2f}")


if __name__ == "__main__":
    main()
