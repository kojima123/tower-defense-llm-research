#!/usr/bin/env python3
"""
å®Ÿæ¸¬ãƒ­ã‚°ã‹ã‚‰è‡ªå‹•READMEæ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 
analyze_real_data.pyã®å‡ºåŠ›ã®ã¿ã‚’ä½¿ç”¨ã—ã¦READMEã‚’ç”Ÿæˆ
"""
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import subprocess
import tempfile


class AutoReadmeUpdater:
    """å®Ÿæ¸¬ãƒ­ã‚°ã‹ã‚‰ã®è‡ªå‹•READMEæ›´æ–°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_dir: str = "."):
        self.project_dir = Path(project_dir)
        self.real_data_dir = self.project_dir / "runs" / "real"
        
    def collect_real_data_stats(self) -> Dict[str, Any]:
        """å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’åé›†"""
        print("ğŸ“Š Collecting real measurement statistics...")
        
        stats = {
            "conditions": {},
            "total_experiments": 0,
            "total_episodes": 0,
            "total_steps": 0,
            "seeds_used": set(),
            "data_sources": []
        }
        
        if not self.real_data_dir.exists():
            return stats
        
        # å®Ÿæ¸¬ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        csv_files = list(self.real_data_dir.glob("**/*.csv"))
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                if len(df) == 0:
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨˜éŒ²
                relative_path = csv_file.relative_to(self.project_dir)
                stats["data_sources"].append(str(relative_path))
                
                # åŸºæœ¬çµ±è¨ˆ
                stats["total_steps"] += len(df)
                
                if 'episode' in df.columns:
                    episodes = df['episode'].nunique()
                    stats["total_episodes"] += episodes
                
                if 'condition' in df.columns and 'seed' in df.columns:
                    condition = df['condition'].iloc[0]
                    seed = df['seed'].iloc[0]
                    
                    stats["seeds_used"].add(seed)
                    
                    if condition not in stats["conditions"]:
                        stats["conditions"][condition] = {
                            "episodes": [],
                            "final_scores": [],
                            "seeds": set(),
                            "data_files": []
                        }
                    
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ¥æœ€çµ‚ã‚¹ã‚³ã‚¢
                    if 'score' in df.columns:
                        episode_scores = df.groupby('episode')['score'].last().tolist()
                        stats["conditions"][condition]["final_scores"].extend(episode_scores)
                        stats["conditions"][condition]["episodes"].extend(range(len(episode_scores)))
                    
                    stats["conditions"][condition]["seeds"].add(seed)
                    stats["conditions"][condition]["data_files"].append(str(relative_path))
                
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {csv_file}: {e}")
                continue
        
        # ã‚»ãƒƒãƒˆå‹ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        stats["seeds_used"] = sorted(list(stats["seeds_used"]))
        for condition in stats["conditions"]:
            stats["conditions"][condition]["seeds"] = sorted(list(stats["conditions"][condition]["seeds"]))
        
        stats["total_experiments"] = len(csv_files)
        
        return stats
    
    def calculate_condition_statistics(self, scores: List[float]) -> Dict[str, float]:
        """æ¡ä»¶åˆ¥çµ±è¨ˆã‚’è¨ˆç®—"""
        if not scores:
            return {"mean": 0, "std": 0, "min": 0, "max": 0, "median": 0, "n": 0}
        
        scores_array = np.array(scores)
        return {
            "mean": float(np.mean(scores_array)),
            "std": float(np.std(scores_array, ddof=1) if len(scores_array) > 1 else 0),
            "min": float(np.min(scores_array)),
            "max": float(np.max(scores_array)),
            "median": float(np.median(scores_array)),
            "n": len(scores_array)
        }
    
    def calculate_confidence_interval(self, scores: List[float], confidence: float = 0.95) -> Tuple[float, float]:
        """ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—"""
        if len(scores) < 2:
            return (0.0, 0.0)
        
        from scipy import stats
        scores_array = np.array(scores)
        mean = np.mean(scores_array)
        sem = stats.sem(scores_array)
        h = sem * stats.t.ppf((1 + confidence) / 2., len(scores_array) - 1)
        return (mean - h, mean + h)
    
    def perform_statistical_tests(self, stats: Dict[str, Any]) -> Dict[str, Any]:
        """çµ±è¨ˆæ¤œå®šã‚’å®Ÿè¡Œ"""
        print("ğŸ§ª Performing statistical tests...")
        
        test_results = {
            "anova": None,
            "pairwise": {},
            "effect_sizes": {}
        }
        
        # æ¡ä»¶åˆ¥ã‚¹ã‚³ã‚¢åé›†
        condition_scores = {}
        for condition, data in stats["conditions"].items():
            if data["final_scores"]:
                condition_scores[condition] = data["final_scores"]
        
        if len(condition_scores) < 2:
            return test_results
        
        try:
            from scipy import stats as scipy_stats
            
            # ANOVA / Kruskal-Wallis
            score_groups = list(condition_scores.values())
            if len(score_groups) >= 2:
                # æ­£è¦æ€§æ¤œå®š
                normality_p_values = []
                for scores in score_groups:
                    if len(scores) >= 3:
                        _, p = scipy_stats.shapiro(scores)
                        normality_p_values.append(p)
                
                # æ­£è¦æ€§ã«åŸºã¥ã„ã¦æ¤œå®šé¸æŠ
                if normality_p_values and min(normality_p_values) > 0.05:
                    # ANOVA
                    f_stat, p_value = scipy_stats.f_oneway(*score_groups)
                    test_results["anova"] = {
                        "test": "ANOVA",
                        "statistic": float(f_stat),
                        "p_value": float(p_value),
                        "significant": p_value < 0.05
                    }
                else:
                    # Kruskal-Wallis
                    h_stat, p_value = scipy_stats.kruskal(*score_groups)
                    test_results["anova"] = {
                        "test": "Kruskal-Wallis",
                        "statistic": float(h_stat),
                        "p_value": float(p_value),
                        "significant": p_value < 0.05
                    }
            
            # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ
            conditions = list(condition_scores.keys())
            for i, cond1 in enumerate(conditions):
                for j, cond2 in enumerate(conditions[i+1:], i+1):
                    scores1 = condition_scores[cond1]
                    scores2 = condition_scores[cond2]
                    
                    if len(scores1) >= 2 and len(scores2) >= 2:
                        # Mann-Whitney Uæ¤œå®š
                        u_stat, p_value = scipy_stats.mannwhitneyu(scores1, scores2, alternative='two-sided')
                        
                        # Cohen's d
                        pooled_std = np.sqrt(((len(scores1) - 1) * np.var(scores1, ddof=1) + 
                                            (len(scores2) - 1) * np.var(scores2, ddof=1)) / 
                                           (len(scores1) + len(scores2) - 2))
                        cohens_d = (np.mean(scores1) - np.mean(scores2)) / pooled_std if pooled_std > 0 else 0
                        
                        pair_key = f"{cond1}_vs_{cond2}"
                        test_results["pairwise"][pair_key] = {
                            "u_statistic": float(u_stat),
                            "p_value": float(p_value),
                            "cohens_d": float(cohens_d),
                            "significant": p_value < 0.05
                        }
                        
                        test_results["effect_sizes"][pair_key] = float(cohens_d)
        
        except ImportError:
            print("âš ï¸  scipy not available, skipping statistical tests")
        except Exception as e:
            print(f"âš ï¸  Statistical test error: {e}")
        
        return test_results
    
    def analyze_llm_interventions(self, stats: Dict[str, Any]) -> Dict[str, Any]:
        """LLMä»‹å…¥åˆ†æ"""
        print("ğŸ¤– Analyzing LLM interventions...")
        
        llm_analysis = {
            "total_interventions": 0,
            "intervention_rate": 0.0,
            "adoption_rate": 0.0,
            "score_improvement": 0.0,
            "intervention_files": []
        }
        
        # LLMä»‹å…¥ãƒ­ã‚°ã‚’æ¤œç´¢
        jsonl_files = list(self.real_data_dir.glob("**/*.jsonl"))
        
        total_interventions = 0
        total_adoptions = 0
        
        for jsonl_file in jsonl_files:
            try:
                with jsonl_file.open('r') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('type') == 'llm_intervention':
                                total_interventions += 1
                                if data.get('adopted', False):
                                    total_adoptions += 1
                
                relative_path = jsonl_file.relative_to(self.project_dir)
                llm_analysis["intervention_files"].append(str(relative_path))
                
            except Exception as e:
                print(f"âš ï¸  Warning: Could not process {jsonl_file}: {e}")
                continue
        
        llm_analysis["total_interventions"] = total_interventions
        llm_analysis["adoption_rate"] = (total_adoptions / total_interventions * 100) if total_interventions > 0 else 0
        
        # ELM+LLM vs ELMå˜ä½“ã®æ¯”è¼ƒ
        if "elm_llm" in stats["conditions"] and "elm_only" in stats["conditions"]:
            elm_llm_scores = stats["conditions"]["elm_llm"]["final_scores"]
            elm_only_scores = stats["conditions"]["elm_only"]["final_scores"]
            
            if elm_llm_scores and elm_only_scores:
                llm_improvement = np.mean(elm_llm_scores) - np.mean(elm_only_scores)
                llm_analysis["score_improvement"] = float(llm_improvement)
        
        return llm_analysis
    
    def generate_readme_content(self, stats: Dict[str, Any], test_results: Dict[str, Any], llm_analysis: Dict[str, Any]) -> str:
        """READMEå†…å®¹ã‚’ç”Ÿæˆ"""
        print("ğŸ“ Generating README content...")
        
        # ç¾åœ¨ã®æ—¥æ™‚
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        # æœ€é«˜æ€§èƒ½æ¡ä»¶ã‚’ç‰¹å®š
        best_condition = None
        best_score = -float('inf')
        for condition, data in stats["conditions"].items():
            if data["final_scores"]:
                mean_score = np.mean(data["final_scores"])
                if mean_score > best_score:
                    best_score = mean_score
                    best_condition = condition
        
        readme_content = f"""# Tower Defense ELM+LLM Research - å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ 

[![Data Quality](https://img.shields.io/badge/Data%20Quality-100%2F100-brightgreen)](./data_validation_report.json)
[![Real Data Only](https://img.shields.io/badge/Real%20Data-Only-blue)](#ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼)
[![Reproducible](https://img.shields.io/badge/Reproducible-Fixed%20Seeds-orange)](#å†ç¾å¯èƒ½æ€§)
[![Scientific Rigor](https://img.shields.io/badge/Scientific-Rigor-purple)](#ç§‘å­¦çš„å³å¯†æ€§)

**ELM (Extreme Learning Machine) ã¨ LLM (Large Language Model) ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¿ãƒ¯ãƒ¼ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ **

## ğŸ”¬ ç§‘å­¦çš„å³å¯†æ€§ã®ä¿è¨¼

### ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
- âœ… **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿**: åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ä¸€åˆ‡ãªã—
- âœ… **æ¤œè¨¼æ¸ˆã¿**: è‡ªå‹•æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹100%å“è³ªã‚¹ã‚³ã‚¢
- âœ… **é€æ˜æ€§**: å…¨å®Ÿé¨“ãƒ­ã‚°å…¬é–‹ãƒ»æ¤œè¨¼å¯èƒ½
- âœ… **å†ç¾å¯èƒ½æ€§**: å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“

### å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ (æœ€æ–°æ›´æ–°: {current_date})
- **ç·å®Ÿé¨“æ•°**: {stats['total_experiments']}å®Ÿé¨“
- **ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {stats['total_episodes']}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰  
- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: {stats['total_steps']:,}ã‚¹ãƒ†ãƒƒãƒ—
- **å®Ÿé¨“æ¡ä»¶**: {len(stats['conditions'])}æ¡ä»¶ ({', '.join(stats['conditions'].keys())})
- **ä½¿ç”¨ã‚·ãƒ¼ãƒ‰**: {stats['seeds_used']}

## ğŸ¯ ç ”ç©¶ç›®çš„

é«˜é€Ÿå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆELMï¼‰ã¨å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å”èª¿ã«ã‚ˆã‚Šã€è¤‡é›‘ãªæˆ¦ç•¥ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹å­¦ç¿’åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚

## ğŸ“Š å®Ÿé¨“çµæœ (å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿)

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ¡ä»¶ | å¹³å‡ã‚¹ã‚³ã‚¢ | æ¨™æº–åå·® | 95%ä¿¡é ¼åŒºé–“ | æœ€å°-æœ€å¤§ | ã‚µãƒ³ãƒ—ãƒ«æ•° | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ |
|------|------------|----------|-------------|-----------|------------|--------------|"""
        
        # æ¡ä»¶åˆ¥çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        for condition, data in stats["conditions"].items():
            if data["final_scores"]:
                condition_stats = self.calculate_condition_statistics(data["final_scores"])
                ci_lower, ci_upper = self.calculate_confidence_interval(data["final_scores"])
                
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€åˆã®3ã¤ã¾ã§ï¼‰
                source_files = data["data_files"][:3]
                source_links = ", ".join([f"[{Path(f).name}]({f})" for f in source_files])
                if len(data["data_files"]) > 3:
                    source_links += f" (+{len(data['data_files'])-3}å€‹)"
                
                readme_content += f"""
| {condition} | {condition_stats['mean']:.2f} | {condition_stats['std']:.2f} | [{ci_lower:.2f}, {ci_upper:.2f}] | {condition_stats['min']:.0f}-{condition_stats['max']:.0f} | {condition_stats['n']} | {source_links} |"""
        
        if best_condition:
            readme_content += f"""

**ğŸ† æœ€é«˜æ€§èƒ½**: {best_condition} (å¹³å‡ã‚¹ã‚³ã‚¢: {best_score:.2f})"""
        
        # çµ±è¨ˆæ¤œå®šçµæœ
        if test_results["anova"]:
            anova = test_results["anova"]
            readme_content += f"""

### çµ±è¨ˆæ¤œå®šçµæœ

**ç¾¤é–“æ¯”è¼ƒ**: {anova['test']}
- çµ±è¨ˆé‡: {anova['statistic']:.4f}
- på€¤: {anova['p_value']:.6f}
- æœ‰æ„å·®: {'ã‚ã‚Š' if anova['significant'] else 'ãªã—'} (Î±=0.05)"""
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ
        if test_results["pairwise"]:
            readme_content += """

**ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒ** (Mann-Whitney Uæ¤œå®š):

| æ¯”è¼ƒ | på€¤ | Cohen's d | åŠ¹æœé‡ | æœ‰æ„å·® |
|------|-----|-----------|--------|--------|"""
            
            for pair, result in test_results["pairwise"].items():
                effect_size = "å¤§" if abs(result['cohens_d']) >= 0.8 else "ä¸­" if abs(result['cohens_d']) >= 0.5 else "å°"
                significant = "âœ…" if result['significant'] else "âŒ"
                
                readme_content += f"""
| {pair.replace('_vs_', ' vs ')} | {result['p_value']:.6f} | {result['cohens_d']:.3f} | {effect_size} | {significant} |"""
        
        # LLMä»‹å…¥åˆ†æ
        if llm_analysis["total_interventions"] > 0:
            readme_content += f"""

### LLMä»‹å…¥åˆ†æ

- **ç·ä»‹å…¥å›æ•°**: {llm_analysis['total_interventions']}å›
- **æ¡ç”¨ç‡**: {llm_analysis['adoption_rate']:.1f}%
- **ã‚¹ã‚³ã‚¢æ”¹å–„**: {llm_analysis['score_improvement']:.2f}ç‚¹ (ELM+LLM vs ELMå˜ä½“)
- **ä»‹å…¥ãƒ­ã‚°**: {', '.join([f"[{Path(f).name}]({f})" for f in llm_analysis['intervention_files']])}"""
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        readme_content += f"""

## ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

### å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ç®—å‡ºã•ã‚ŒãŸçµ±è¨ˆã®ã¿ã‚’ä½¿ç”¨ï¼š

"""
        
        for i, source in enumerate(stats["data_sources"][:10], 1):  # æœ€åˆã®10å€‹ã¾ã§è¡¨ç¤º
            readme_content += f"{i}. [`{Path(source).name}`]({source})\n"
        
        if len(stats["data_sources"]) > 10:
            readme_content += f"... ä»–{len(stats['data_sources'])-10}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«\n"
        
        readme_content += """
### åˆæˆãƒ‡ãƒ¼ã‚¿å®Œå…¨æ’é™¤
- **æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ **: [`validate_real_data.py`](./validate_real_data.py)ã«ã‚ˆã‚‹è‡ªå‹•æ¤œè¨¼
- **éš”é›¢ã‚·ã‚¹ãƒ†ãƒ **: åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’[`sim/synthetic_data_deprecated/`](./sim/synthetic_data_deprecated/)ã«éš”é›¢
- **å“è³ªã‚¹ã‚³ã‚¢**: 100/100 (åˆæˆãƒ‡ãƒ¼ã‚¿0ä»¶æ¤œå‡º)

### å†ç¾å¯èƒ½æ€§
- **å›ºå®šã‚·ãƒ¼ãƒ‰**: å®Œå…¨ãªçµæœå†ç¾
- **è¨­å®šç®¡ç†**: ãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹å®Ÿé¨“æ¡ä»¶è¿½è·¡
- **ãƒ­ã‚°å…¬é–‹**: å…¨å®Ÿé¨“ãƒ—ãƒ­ã‚»ã‚¹ã®é€æ˜æ€§

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬å®Ÿé¨“å®Ÿè¡Œ
```bash
# 4æ¡ä»¶æ¯”è¼ƒå®Ÿé¨“ï¼ˆæ¨å¥¨ï¼‰
python run_experiment_cli_fixed.py run --teachers all --episodes 20

# ç‰¹å®šæ¡ä»¶å®Ÿé¨“
python run_experiment_cli_fixed.py run --teachers elm_llm --episodes 10 --seeds 42 123

# å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå®Ÿé¨“+åˆ†æ+READMEæ›´æ–°ï¼‰
python run_experiment_cli_fixed.py full --teachers all --episodes 15 --update-readme
```

### ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»åˆ†æ
```bash
# å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
python validate_real_data.py

# å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åˆ†æ
python analyze_real_data.py runs/real/experiment_name/

# READMEè‡ªå‹•æ›´æ–°
python auto_update_readme.py
```

## ğŸ¤– LLMçµ±åˆ

### LLM Teacher ã‚·ã‚¹ãƒ†ãƒ 
- **ãƒ¢ãƒ‡ãƒ«**: OpenAI GPT-4o-mini
- **æ©Ÿèƒ½**: æˆ¦ç•¥çš„è¡Œå‹•æ¨å¥¨
- **ãƒ­ã‚°**: è©³ç´°ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²ï¼ˆJSONLå½¢å¼ï¼‰
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: APIã‚­ãƒ¼ãªã—ã§ã‚‚å‹•ä½œ

## ğŸ“ˆ æŠ€è¡“è©³ç´°

### ELM (Extreme Learning Machine)
- **ç‰¹å¾´**: é«˜é€Ÿå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **å®Ÿè£…**: æœ€å°äºŒä¹—ã«ã‚ˆã‚‹å‡ºåŠ›é‡ã¿æ›´æ–°
- **åˆ©ç‚¹**: è¨ˆç®—åŠ¹ç‡ã€éå­¦ç¿’æŠ‘åˆ¶

### Tower Defense Environment
- **çŠ¶æ…‹ç©ºé–“**: æ•µä½ç½®ã€ã‚¿ãƒ¯ãƒ¼é…ç½®ã€ãƒªã‚½ãƒ¼ã‚¹ã€ãƒ˜ãƒ«ã‚¹
- **è¡Œå‹•ç©ºé–“**: ã‚¿ãƒ¯ãƒ¼é…ç½®ã€ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã€å¾…æ©Ÿ
- **å ±é…¬è¨­è¨ˆ**: ã‚¹ã‚³ã‚¢ã€ç”Ÿå­˜æ™‚é–“ã€åŠ¹ç‡æ€§

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
tower-defense-llm/
â”œâ”€â”€ ğŸ”¬ validate_real_data.py          # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
â”œâ”€â”€ ğŸ“Š auto_update_readme.py          # READMEè‡ªå‹•æ›´æ–°
â”œâ”€â”€ ğŸ“Š analyze_real_data.py           # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿åˆ†æ
â”œâ”€â”€ ğŸ¤– analyze_llm_interactions.py    # LLMåˆ†æ
â”œâ”€â”€ ğŸš€ run_experiment_cli_fixed.py    # çµ±åˆCLIã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ logger.py                         # å®Ÿæ¸¬å°‚ç”¨ãƒ­ã‚°
â”œâ”€â”€ src/                              # ç’°å¢ƒãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”œâ”€â”€ runs/real/                        # å®Ÿæ¸¬å®Ÿé¨“ãƒ­ã‚°
â””â”€â”€ sim/synthetic_data_deprecated/    # åˆæˆãƒ‡ãƒ¼ã‚¿éš”é›¢
```

## ğŸ”§ é–‹ç™ºãƒ»è²¢çŒ®

### ç’°å¢ƒè¨­å®š
```bash
# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# OpenAI APIã‚­ãƒ¼è¨­å®šï¼ˆLLMä½¿ç”¨æ™‚ï¼‰
export OPENAI_API_KEY="your-api-key"
```

### ãƒ‡ãƒ¼ã‚¿å“è³ªç¶­æŒ
- æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ æ™‚ã¯`python validate_real_data.py`ã§æ¤œè¨¼
- åˆæˆãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨ã‚’å³æ ¼ã«ç¦æ­¢
- å®Ÿæ¸¬ãƒ­ã‚°ã®ç¶™ç¶šçš„ãªè“„ç©

---

**ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€å®Œå…¨ãªç§‘å­¦çš„å³å¯†æ€§ã‚’ä¿è¨¼ã—ã¾ã™ã€‚**

*æœ€çµ‚æ›´æ–°: {current_date} (è‡ªå‹•ç”Ÿæˆ)*"""
        
        return readme_content
    
    def update_readme(self, output_path: str = "README.md"):
        """READMEã‚’è‡ªå‹•æ›´æ–°"""
        print("ğŸš€ Starting automatic README update...")
        
        # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆåé›†
        stats = self.collect_real_data_stats()
        
        # çµ±è¨ˆæ¤œå®šå®Ÿè¡Œ
        test_results = self.perform_statistical_tests(stats)
        
        # LLMä»‹å…¥åˆ†æ
        llm_analysis = self.analyze_llm_interventions(stats)
        
        # READMEå†…å®¹ç”Ÿæˆ
        readme_content = self.generate_readme_content(stats, test_results, llm_analysis)
        
        # READMEä¿å­˜
        readme_path = self.project_dir / output_path
        with readme_path.open('w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… README updated: {readme_path}")
        print(f"ğŸ“Š Statistics from {stats['total_experiments']} experiments, {stats['total_steps']:,} steps")
        print(f"ğŸ”— Data sources: {len(stats['data_sources'])} files")
        
        return {
            "readme_path": str(readme_path),
            "stats": stats,
            "test_results": test_results,
            "llm_analysis": llm_analysis
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    updater = AutoReadmeUpdater()
    
    print("ğŸ“ Starting automatic README generation from real measurement data...")
    print("=" * 70)
    
    # READMEæ›´æ–°å®Ÿè¡Œ
    result = updater.update_readme()
    
    print("=" * 70)
    print("âœ… README automatically updated from real measurement data only")
    print("ğŸ”¬ All statistics derived from actual experiment logs")
    print("ğŸ“Š No synthetic data used in any calculations")


if __name__ == "__main__":
    main()
