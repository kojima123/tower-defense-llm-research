#!/usr/bin/env python3
"""
å®Œå…¨å®Ÿé¨“ãƒ©ãƒ³ãƒŠãƒ¼
4æ¡ä»¶æ¯”è¼ƒ + LLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ­ã‚° + åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""
import argparse
import time
import json
import os
from pathlib import Path
from run_fixed_seed_experiments import FixedSeedExperimentRunner
from analyze_llm_interactions import LLMInteractionAnalyzer


class CompleteExperimentRunner:
    """å®Œå…¨å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir: str = "runs/real"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # å®Ÿé¨“è¨­å®š
        self.conditions = ["elm_only", "rule_teacher", "random_teacher", "elm_llm"]
        self.seeds = [42, 123, 456]
        
        # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
        self.openai_available = bool(os.getenv('OPENAI_API_KEY'))
        if not self.openai_available:
            print("âš ï¸  Warning: OPENAI_API_KEY not set. ELM+LLM experiments will use fallback mode.")
            print("   Set OPENAI_API_KEY environment variable for full LLM functionality.")
    
    def run_complete_experiment(self, episodes_per_seed: int = 10, parallel: bool = False):
        """å®Œå…¨å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ Starting complete Tower Defense experiment...")
        print(f"ğŸ“Š Conditions: {self.conditions}")
        print(f"ğŸ² Seeds: {self.seeds}")
        print(f"ğŸ“ˆ Episodes per seed: {episodes_per_seed}")
        print(f"ğŸ”‘ OpenAI API: {'Available' if self.openai_available else 'Fallback mode'}")
        print(f"âš¡ Parallel execution: {parallel}")
        print("-" * 80)
        
        start_time = time.time()
        
        # å›ºå®šã‚·ãƒ¼ãƒ‰å®Ÿé¨“å®Ÿè¡Œ
        runner = FixedSeedExperimentRunner(str(self.base_dir))
        
        if parallel:
            summary = runner.run_all_conditions_parallel(episodes_per_seed)
        else:
            summary = runner.run_all_conditions_sequential(episodes_per_seed)
        
        # å®Ÿé¨“æ•´åˆæ€§æ¤œè¨¼
        integrity_ok = runner.validate_experiment_integrity()
        
        # LLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³åˆ†æï¼ˆELM+LLMæ¡ä»¶ã®ã¿ï¼‰
        llm_analysis_results = self.analyze_llm_interactions()
        
        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        total_time = time.time() - start_time
        comprehensive_report = self.generate_comprehensive_report(
            summary, llm_analysis_results, total_time, integrity_ok
        )
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ COMPLETE EXPERIMENT FINISHED")
        print(f"â±ï¸  Total time: {total_time:.2f}s")
        print(f"âœ… Integrity check: {'PASSED' if integrity_ok else 'FAILED'}")
        print(f"ğŸ“ Results directory: {self.base_dir}")
        print(f"ğŸ“Š Comprehensive report: {self.base_dir}/comprehensive_experiment_report.md")
        print(f"{'='*80}")
        
        return comprehensive_report
    
    def analyze_llm_interactions(self):
        """å…¨ã¦ã®LLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†æ"""
        print("\nğŸ” Analyzing LLM interactions...")
        
        llm_results = {}
        
        # ELM+LLMæ¡ä»¶ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢
        elm_llm_dir = self.base_dir / "elm_llm"
        
        if not elm_llm_dir.exists():
            print("   âš ï¸  No ELM+LLM experiment data found")
            return llm_results
        
        # å„ã‚·ãƒ¼ãƒ‰ã®LLMãƒ­ã‚°ã‚’åˆ†æ
        for seed_dir in elm_llm_dir.glob("seed_*"):
            if seed_dir.is_dir():
                seed_name = seed_dir.name
                print(f"   ğŸ“Š Analyzing {seed_name}...")
                
                try:
                    analyzer = LLMInteractionAnalyzer(str(seed_dir))
                    
                    # åˆ†æå®Ÿè¡Œ
                    patterns = analyzer.analyze_interaction_patterns()
                    actions = analyzer.extract_action_recommendations()
                    adoption = analyzer.analyze_adoption_patterns()
                    
                    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                    analyzer.generate_interaction_report()
                    analyzer.create_visualization()
                    
                    llm_results[seed_name] = {
                        "patterns": patterns,
                        "actions": actions,
                        "adoption": adoption
                    }
                    
                    print(f"     âœ… {patterns.get('total_interactions', 0)} interactions analyzed")
                    
                except Exception as e:
                    print(f"     âŒ Analysis failed: {e}")
                    llm_results[seed_name] = {"error": str(e)}
        
        return llm_results
    
    def generate_comprehensive_report(self, experiment_summary, llm_analysis, total_time, integrity_ok):
        """ç·åˆå®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        report_file = self.base_dir / "comprehensive_experiment_report.md"
        
        # å®Ÿé¨“çµæœã®çµ±è¨ˆè¨ˆç®—
        condition_stats = {}
        for condition, data in experiment_summary.get("results", {}).items():
            if "results" in data and data["results"]:
                scores = [r["mean_score"] for r in data["results"]]
                condition_stats[condition] = {
                    "mean_score": sum(scores) / len(scores),
                    "std_score": (sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores))**0.5,
                    "min_score": min(scores),
                    "max_score": max(scores),
                    "sample_size": len(scores)
                }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = f"""# Tower Defense ELM+LLM å®Œå…¨å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿé¨“æ¦‚è¦

- **å®Ÿè¡Œæ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **ç·å®Ÿè¡Œæ™‚é–“**: {total_time:.2f}ç§’
- **å®Ÿé¨“æ¡ä»¶**: {len(self.conditions)}æ¡ä»¶
- **ä½¿ç”¨ã‚·ãƒ¼ãƒ‰**: {self.seeds}
- **ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°**: {experiment_summary.get('episodes_per_seed', 0)} Ã— {len(self.seeds)} = {experiment_summary.get('total_episodes_per_condition', 0)}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/æ¡ä»¶
- **ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§**: {'âœ… PASSED' if integrity_ok else 'âŒ FAILED'}
- **OpenAI API**: {'âœ… Available' if self.openai_available else 'âš ï¸ Fallback mode'}

## å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼

### æ¡ä»¶åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

"""
        
        for condition, stats in condition_stats.items():
            report += f"""#### {condition.upper()}
- **å¹³å‡ã‚¹ã‚³ã‚¢**: {stats['mean_score']:.2f} Â± {stats['std_score']:.2f}
- **ã‚¹ã‚³ã‚¢ç¯„å›²**: {stats['min_score']:.0f} - {stats['max_score']:.0f}
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: {stats['sample_size']}

"""
        
        # LLMåˆ†æçµæœ
        if llm_analysis:
            report += """## LLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³åˆ†æ

### å…¨ä½“çµ±è¨ˆ

"""
            total_interactions = 0
            total_adoption_rate = 0
            seed_count = 0
            
            for seed_name, analysis in llm_analysis.items():
                if "error" not in analysis:
                    patterns = analysis.get("patterns", {})
                    adoption = analysis.get("adoption", {})
                    
                    interactions = patterns.get("total_interactions", 0)
                    adoption_rate = adoption.get("overall_adoption_rate", 0)
                    
                    total_interactions += interactions
                    total_adoption_rate += adoption_rate
                    seed_count += 1
                    
                    report += f"""#### {seed_name}
- **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°**: {interactions}
- **æ¡ç”¨ç‡**: {adoption_rate:.2%}
- **ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: {patterns.get('unique_prompts', 0)}

"""
            
            if seed_count > 0:
                avg_adoption = total_adoption_rate / seed_count
                report += f"""### çµ±åˆçµ±è¨ˆ
- **ç·ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°**: {total_interactions}
- **å¹³å‡æ¡ç”¨ç‡**: {avg_adoption:.2%}
- **åˆ†æå¯¾è±¡ã‚·ãƒ¼ãƒ‰**: {seed_count}

"""
        
        # å®Ÿé¨“è¨­å®šè©³ç´°
        report += f"""## å®Ÿé¨“è¨­å®šè©³ç´°

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ
- **ELMéš ã‚Œå±¤ã‚µã‚¤ã‚º**: 100
- **ç’°å¢ƒãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0

### å®Ÿè¡Œç’°å¢ƒ
- **å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰**: {'ä¸¦åˆ—' if experiment_summary.get('max_workers') else 'é †æ¬¡'}
- **ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª**: {self.base_dir}
- **ãƒ­ã‚°å½¢å¼**: CSV + JSON + JSONL

### ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
- **åˆæˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨**: âŒ ãªã—ï¼ˆå®Ÿæ¸¬ã®ã¿ï¼‰
- **å›ºå®šã‚·ãƒ¼ãƒ‰**: âœ… å®Œå…¨å†ç¾å¯èƒ½
- **è¨­å®šãƒãƒƒã‚·ãƒ¥**: âœ… è‡ªå‹•ç”Ÿæˆ
- **ãƒ­ã‚°æ•´åˆæ€§**: {'âœ… æ¤œè¨¼æ¸ˆã¿' if integrity_ok else 'âŒ å•é¡Œã‚ã‚Š'}

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
{self.base_dir}/
â”œâ”€â”€ elm_only/
â”‚   â”œâ”€â”€ seed_42/
â”‚   â”œâ”€â”€ seed_123/
â”‚   â””â”€â”€ seed_456/
â”œâ”€â”€ rule_teacher/
â”‚   â”œâ”€â”€ seed_42/
â”‚   â”œâ”€â”€ seed_123/
â”‚   â””â”€â”€ seed_456/
â”œâ”€â”€ random_teacher/
â”‚   â”œâ”€â”€ seed_42/
â”‚   â”œâ”€â”€ seed_123/
â”‚   â””â”€â”€ seed_456/
â”œâ”€â”€ elm_llm/
â”‚   â”œâ”€â”€ seed_42/
â”‚   â”œâ”€â”€ seed_123/
â”‚   â””â”€â”€ seed_456/
â””â”€â”€ experiment_summary.json
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **çµ±è¨ˆåˆ†æ**: scipy.statsã‚’ä½¿ç”¨ã—ãŸæœ‰æ„å·®æ¤œå®š
2. **å¯è¦–åŒ–**: matplotlib/seabornã«ã‚ˆã‚‹çµæœå¯è¦–åŒ–
3. **è«–æ–‡åŸ·ç­†**: å®Ÿé¨“çµæœã®å­¦è¡“çš„è¨˜è¿°
4. **å†ç¾æ€§æ¤œè¨¼**: ç•°ãªã‚‹ç’°å¢ƒã§ã®å®Ÿé¨“å†å®Ÿè¡Œ

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«åŸºã¥ã„ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with report_file.open('w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“Š Comprehensive report saved to: {report_file}")
        
        return report


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Run complete Tower Defense experiment with LLM analysis")
    parser.add_argument("--episodes", type=int, default=10, 
                       help="Episodes per seed (default: 10)")
    parser.add_argument("--parallel", action="store_true", 
                       help="Run experiments in parallel")
    parser.add_argument("--base_dir", type=str, default="runs/real/complete", 
                       help="Base directory for results")
    
    args = parser.parse_args()
    
    # å®Œå…¨å®Ÿé¨“å®Ÿè¡Œ
    runner = CompleteExperimentRunner(args.base_dir)
    runner.run_complete_experiment(args.episodes, args.parallel)


if __name__ == "__main__":
    main()
