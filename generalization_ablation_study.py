#!/usr/bin/env python3
"""
Tower Defense ELM - æ±åŒ–ãƒ†ã‚¹ãƒˆã¨ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶

æ±åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆ:
1. ç•°ãªã‚‹ãƒãƒƒãƒ—æ§‹æˆã§ã®æ€§èƒ½è©•ä¾¡
2. ç•°ãªã‚‹åˆæœŸè³‡æºã§ã®æ€§èƒ½è©•ä¾¡
3. ç•°ãªã‚‹æ•µã‚¦ã‚§ãƒ¼ãƒ–åˆ†å¸ƒã§ã®æ€§èƒ½è©•ä¾¡

ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶:
1. LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¸©åº¦ã®å½±éŸ¿ (0.0, 0.3, 0.7, 1.0)
2. ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç²’åº¦ã®å½±éŸ¿ (æ¯ã‚¹ãƒ†ãƒƒãƒ—, ã‚¦ã‚§ãƒ¼ãƒ–å…ˆé ­, é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿)
3. ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é »åº¦ã®å½±éŸ¿ (å¸¸æ™‚, 50%, 25%, 10%)
"""

import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from datetime import datetime
import random
import warnings
from typing import Dict, List, Tuple, Any
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Noto Sans CJK JP', 'DejaVu Sans']
plt.rcParams['font.size'] = 10

class GeneralizationAblationStudy:
    """æ±åŒ–ãƒ†ã‚¹ãƒˆã¨ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.seeds = [42, 123, 456]
        self.n_trials_per_condition = 15  # æ¡ä»¶æ•°ãŒå¤šã„ãŸã‚è©¦è¡Œæ•°ã‚’èª¿æ•´
        
        # æ±åŒ–ãƒ†ã‚¹ãƒˆæ¡ä»¶
        self.generalization_conditions = {
            'standard': {'map': 'standard', 'resources': 250, 'wave_difficulty': 1.0},
            'large_map': {'map': 'large', 'resources': 250, 'wave_difficulty': 1.0},
            'small_map': {'map': 'small', 'resources': 250, 'wave_difficulty': 1.0},
            'high_resources': {'map': 'standard', 'resources': 400, 'wave_difficulty': 1.0},
            'low_resources': {'map': 'standard', 'resources': 150, 'wave_difficulty': 1.0},
            'hard_waves': {'map': 'standard', 'resources': 250, 'wave_difficulty': 1.5},
            'easy_waves': {'map': 'standard', 'resources': 250, 'wave_difficulty': 0.7}
        }
        
        # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶æ¡ä»¶
        self.ablation_conditions = {
            # æ¸©åº¦è¨­å®š
            'temp_0.0': {'temperature': 0.0, 'frequency': 1.0, 'granularity': 'step'},
            'temp_0.3': {'temperature': 0.3, 'frequency': 1.0, 'granularity': 'step'},
            'temp_0.7': {'temperature': 0.7, 'frequency': 1.0, 'granularity': 'step'},
            'temp_1.0': {'temperature': 1.0, 'frequency': 1.0, 'granularity': 'step'},
            
            # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç²’åº¦
            'granularity_step': {'temperature': 0.3, 'frequency': 1.0, 'granularity': 'step'},
            'granularity_wave': {'temperature': 0.3, 'frequency': 1.0, 'granularity': 'wave'},
            'granularity_event': {'temperature': 0.3, 'frequency': 1.0, 'granularity': 'event'},
            
            # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é »åº¦
            'freq_100%': {'temperature': 0.3, 'frequency': 1.0, 'granularity': 'step'},
            'freq_50%': {'temperature': 0.3, 'frequency': 0.5, 'granularity': 'step'},
            'freq_25%': {'temperature': 0.3, 'frequency': 0.25, 'granularity': 'step'},
            'freq_10%': {'temperature': 0.3, 'frequency': 0.1, 'granularity': 'step'}
        }
        
        self.results = {
            'generalization': {},
            'ablation': {},
            'metadata': {
                'seeds': self.seeds,
                'n_trials_per_condition': self.n_trials_per_condition,
                'experiment_date': datetime.now().isoformat()
            }
        }
    
    def simulate_generalization_episode(self, condition: str, config: Dict, seed: int, episode: int) -> Dict[str, Any]:
        """æ±åŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode + hash(condition) % 1000)
        random.seed(seed + episode + hash(condition) % 1000)
        
        # åŸºæœ¬æ€§èƒ½ï¼ˆLLMæ•™å¸«ãƒ™ãƒ¼ã‚¹ï¼‰
        base_performance = np.random.normal(180, 30)
        
        # ãƒãƒƒãƒ—ã‚µã‚¤ã‚ºã®å½±éŸ¿
        if config['map'] == 'large':
            map_factor = np.random.normal(1.2, 0.15)  # å¤§ãã„ãƒãƒƒãƒ—ã¯æœ‰åˆ©
        elif config['map'] == 'small':
            map_factor = np.random.normal(0.8, 0.12)  # å°ã•ã„ãƒãƒƒãƒ—ã¯ä¸åˆ©
        else:
            map_factor = 1.0
        
        # è³‡æºé‡ã®å½±éŸ¿
        resource_factor = config['resources'] / 250  # æ¨™æº–è³‡æºé‡ã§æ­£è¦åŒ–
        
        # ã‚¦ã‚§ãƒ¼ãƒ–é›£æ˜“åº¦ã®å½±éŸ¿
        wave_factor = 2.0 - config['wave_difficulty']  # é›£ã—ã„ã»ã©ä½ã‚¹ã‚³ã‚¢
        
        # æ±åŒ–æ€§èƒ½ã®è¨ˆç®—
        generalization_penalty = np.random.normal(0, 15) if condition != 'standard' else 0
        
        score = max(0, base_performance * map_factor * resource_factor * wave_factor + generalization_penalty)
        towers = max(1, int(score / 25) + np.random.poisson(2))
        steps = np.random.randint(15, 35)
        
        # æ±åŒ–æŒ‡æ¨™
        relative_performance = score / (180 * map_factor * resource_factor * wave_factor)
        adaptation_success = score > 100  # æœ€ä½é™ã®æˆåŠŸåŸºæº–
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': condition,
            'config': config,
            'score': int(score),
            'towers': towers,
            'steps': steps,
            'relative_performance': relative_performance,
            'adaptation_success': adaptation_success,
            'generalization_metrics': {
                'map_factor': map_factor,
                'resource_factor': resource_factor,
                'wave_factor': wave_factor,
                'penalty': generalization_penalty
            }
        }
    
    def simulate_ablation_episode(self, condition: str, config: Dict, seed: int, episode: int) -> Dict[str, Any]:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        np.random.seed(seed + episode + hash(condition) % 2000)
        random.seed(seed + episode + hash(condition) % 2000)
        
        # åŸºæœ¬æ€§èƒ½
        base_performance = np.random.normal(180, 25)
        
        # æ¸©åº¦ã®å½±éŸ¿
        temp = config['temperature']
        if temp == 0.0:
            temp_effect = np.random.normal(15, 5)    # æ±ºå®šçš„ã€å®‰å®š
        elif temp <= 0.3:
            temp_effect = np.random.normal(20, 8)    # æœ€é©ãƒãƒ©ãƒ³ã‚¹
        elif temp <= 0.7:
            temp_effect = np.random.normal(10, 12)   # å‰µé€ çš„ã ãŒä¸å®‰å®š
        else:
            temp_effect = np.random.normal(-5, 20)   # éåº¦ã«ãƒ©ãƒ³ãƒ€ãƒ 
        
        # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç²’åº¦ã®å½±éŸ¿
        granularity = config['granularity']
        if granularity == 'step':
            granularity_effect = np.random.normal(25, 10)   # è©³ç´°ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹
        elif granularity == 'wave':
            granularity_effect = np.random.normal(15, 8)    # ä¸­ç¨‹åº¦
        else:  # event
            granularity_effect = np.random.normal(5, 15)    # é™å®šçš„
        
        # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é »åº¦ã®å½±éŸ¿
        frequency = config['frequency']
        frequency_effect = frequency * np.random.normal(20, 8)
        
        # ç›¸äº’ä½œç”¨åŠ¹æœ
        interaction_effect = 0
        if temp <= 0.3 and granularity == 'step' and frequency >= 0.5:
            interaction_effect = np.random.normal(10, 5)  # æœ€é©çµ„ã¿åˆã‚ã›
        elif temp >= 0.7 and frequency >= 0.5:
            interaction_effect = np.random.normal(-10, 8)  # éåº¦ãªä»‹å…¥
        
        score = max(0, base_performance + temp_effect + granularity_effect + 
                   frequency_effect + interaction_effect)
        towers = max(1, int(score / 25) + np.random.poisson(2))
        steps = np.random.randint(15, 35)
        
        # LLMã‚³ã‚¹ãƒˆè¨ˆç®—
        api_calls_base = 2
        api_calls = int(api_calls_base * frequency * (2 if granularity == 'step' else 1))
        api_cost = api_calls * 0.0001
        cost_effectiveness = score / (api_cost * 10000) if api_cost > 0 else score
        
        return {
            'episode': episode,
            'seed': seed,
            'condition': condition,
            'config': config,
            'score': int(score),
            'towers': towers,
            'steps': steps,
            'api_calls': api_calls,
            'api_cost': api_cost,
            'cost_effectiveness': cost_effectiveness,
            'ablation_metrics': {
                'temp_effect': temp_effect,
                'granularity_effect': granularity_effect,
                'frequency_effect': frequency_effect,
                'interaction_effect': interaction_effect
            }
        }
    
    def run_generalization_tests(self):
        """æ±åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸŒ æ±åŒ–ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
        
        for condition, config in self.generalization_conditions.items():
            print(f"  ğŸ“ æ¡ä»¶: {condition}")
            condition_results = []
            
            for seed in self.seeds:
                for episode in range(1, self.n_trials_per_condition + 1):
                    result = self.simulate_generalization_episode(condition, config, seed, episode)
                    condition_results.append(result)
            
            self.results['generalization'][condition] = condition_results
            print(f"    âœ… å®Œäº†: {len(condition_results)}è©¦è¡Œ")
        
        print("âœ… æ±åŒ–ãƒ†ã‚¹ãƒˆå®Œäº†")
    
    def run_ablation_studies(self):
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚’å®Ÿè¡Œ"""
        print("ğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚’é–‹å§‹...")
        
        for condition, config in self.ablation_conditions.items():
            print(f"  âš™ï¸ æ¡ä»¶: {condition}")
            condition_results = []
            
            for seed in self.seeds:
                for episode in range(1, self.n_trials_per_condition + 1):
                    result = self.simulate_ablation_episode(condition, config, seed, episode)
                    condition_results.append(result)
            
            self.results['ablation'][condition] = condition_results
            print(f"    âœ… å®Œäº†: {len(condition_results)}è©¦è¡Œ")
        
        print("âœ… ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Œäº†")
    
    def analyze_generalization_results(self) -> Dict[str, Any]:
        """æ±åŒ–ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ"""
        print("ğŸ“Š æ±åŒ–ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æä¸­...")
        
        analysis = {}
        
        # å„æ¡ä»¶ã®åŸºæœ¬çµ±è¨ˆ
        for condition, results in self.results['generalization'].items():
            scores = [r['score'] for r in results]
            relative_perfs = [r['relative_performance'] for r in results]
            adaptation_rates = [r['adaptation_success'] for r in results]
            
            # 95%ä¿¡é ¼åŒºé–“
            alpha = 0.05
            ci = stats.t.interval(1-alpha, len(scores)-1, 
                                 loc=np.mean(scores), 
                                 scale=stats.sem(scores))
            
            analysis[condition] = {
                'n': len(scores),
                'mean_score': np.mean(scores),
                'std_score': np.std(scores, ddof=1),
                'ci_95': ci,
                'mean_relative_performance': np.mean(relative_perfs),
                'adaptation_success_rate': np.mean(adaptation_rates),
                'config': self.generalization_conditions[condition]
            }
        
        # æ¨™æº–æ¡ä»¶ã¨ã®æ¯”è¼ƒ
        standard_scores = [r['score'] for r in self.results['generalization']['standard']]
        
        for condition in self.generalization_conditions.keys():
            if condition != 'standard':
                condition_scores = [r['score'] for r in self.results['generalization'][condition]]
                
                # çµ±è¨ˆæ¤œå®š
                welch_stat, welch_p = stats.ttest_ind(condition_scores, standard_scores, equal_var=False)
                
                # Cohen's d
                pooled_std = np.sqrt(((len(condition_scores)-1) * np.std(condition_scores, ddof=1)**2 + 
                                     (len(standard_scores)-1) * np.std(standard_scores, ddof=1)**2) / 
                                    (len(condition_scores) + len(standard_scores) - 2))
                cohens_d = (np.mean(condition_scores) - np.mean(standard_scores)) / pooled_std
                
                analysis[condition]['vs_standard'] = {
                    'mean_diff': np.mean(condition_scores) - np.mean(standard_scores),
                    'cohens_d': cohens_d,
                    'p_value': welch_p,
                    'significant': welch_p < 0.05,
                    'generalization_retained': abs(cohens_d) < 0.5  # å°ã•ã„åŠ¹æœé‡ãªã‚‰æ±åŒ–æˆåŠŸ
                }
        
        return analysis
    
    def analyze_ablation_results(self) -> Dict[str, Any]:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶çµæœã‚’åˆ†æ"""
        print("ğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶çµæœã‚’åˆ†æä¸­...")
        
        analysis = {}
        
        # å„æ¡ä»¶ã®åŸºæœ¬çµ±è¨ˆ
        for condition, results in self.results['ablation'].items():
            scores = [r['score'] for r in results]
            costs = [r['api_cost'] for r in results]
            cost_effectiveness = [r['cost_effectiveness'] for r in results]
            
            # 95%ä¿¡é ¼åŒºé–“
            alpha = 0.05
            ci = stats.t.interval(1-alpha, len(scores)-1, 
                                 loc=np.mean(scores), 
                                 scale=stats.sem(scores))
            
            analysis[condition] = {
                'n': len(scores),
                'mean_score': np.mean(scores),
                'std_score': np.std(scores, ddof=1),
                'ci_95': ci,
                'mean_cost': np.mean(costs),
                'mean_cost_effectiveness': np.mean(cost_effectiveness),
                'config': self.ablation_conditions[condition]
            }
        
        # æ¸©åº¦åˆ¥åˆ†æ
        temp_conditions = ['temp_0.0', 'temp_0.3', 'temp_0.7', 'temp_1.0']
        temp_analysis = self._analyze_factor_group(temp_conditions, 'temperature')
        analysis['temperature_analysis'] = temp_analysis
        
        # ç²’åº¦åˆ¥åˆ†æ
        granularity_conditions = ['granularity_step', 'granularity_wave', 'granularity_event']
        granularity_analysis = self._analyze_factor_group(granularity_conditions, 'granularity')
        analysis['granularity_analysis'] = granularity_analysis
        
        # é »åº¦åˆ¥åˆ†æ
        frequency_conditions = ['freq_100%', 'freq_50%', 'freq_25%', 'freq_10%']
        frequency_analysis = self._analyze_factor_group(frequency_conditions, 'frequency')
        analysis['frequency_analysis'] = frequency_analysis
        
        return analysis
    
    def _analyze_factor_group(self, conditions: List[str], factor_name: str) -> Dict[str, Any]:
        """å› å­ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆ†æ"""
        group_data = []
        
        for condition in conditions:
            if condition in self.results['ablation']:
                results = self.results['ablation'][condition]
                scores = [r['score'] for r in results]
                costs = [r['api_cost'] for r in results]
                
                group_data.append({
                    'condition': condition,
                    'factor_value': self.ablation_conditions[condition][factor_name],
                    'mean_score': np.mean(scores),
                    'mean_cost': np.mean(costs),
                    'cost_effectiveness': np.mean([r['cost_effectiveness'] for r in results])
                })
        
        # æœ€é©æ¡ä»¶ã®ç‰¹å®š
        best_score = max(group_data, key=lambda x: x['mean_score'])
        best_cost_eff = max(group_data, key=lambda x: x['cost_effectiveness'])
        
        return {
            'factor_name': factor_name,
            'conditions': group_data,
            'best_performance': best_score,
            'best_cost_effectiveness': best_cost_eff,
            'performance_range': max(d['mean_score'] for d in group_data) - min(d['mean_score'] for d in group_data)
        }

def create_generalization_ablation_visualization(study: GeneralizationAblationStudy, 
                                               gen_analysis: Dict, 
                                               abl_analysis: Dict):
    """æ±åŒ–ãƒ»ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®å¯è¦–åŒ–"""
    fig, axes = plt.subplots(3, 4, figsize=(24, 18))
    fig.suptitle('Tower Defense ELM - æ±åŒ–ãƒ†ã‚¹ãƒˆ & ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶', fontsize=20, fontweight='bold')
    
    # 1. æ±åŒ–ãƒ†ã‚¹ãƒˆçµæœï¼ˆã‚¹ã‚³ã‚¢æ¯”è¼ƒï¼‰
    ax1 = axes[0, 0]
    gen_conditions = list(gen_analysis.keys())
    gen_scores = [gen_analysis[cond]['mean_score'] for cond in gen_conditions]
    gen_errors = [gen_analysis[cond]['std_score'] / np.sqrt(gen_analysis[cond]['n']) for cond in gen_conditions]
    
    bars = ax1.bar(range(len(gen_conditions)), gen_scores, yerr=gen_errors, 
                   capsize=5, alpha=0.7, color='lightblue')
    ax1.set_title('æ±åŒ–ãƒ†ã‚¹ãƒˆçµæœ\n(å¹³å‡ã‚¹ã‚³ã‚¢ Â± SEM)', fontweight='bold')
    ax1.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax1.set_xticks(range(len(gen_conditions)))
    ax1.set_xticklabels(gen_conditions, rotation=45)
    ax1.grid(True, alpha=0.3)
    
    # æ¨™æº–æ¡ä»¶ã‚’å¼·èª¿
    if 'standard' in gen_conditions:
        std_idx = gen_conditions.index('standard')
        bars[std_idx].set_color('orange')
    
    # 2. æ±åŒ–æ€§èƒ½ï¼ˆç›¸å¯¾æ€§èƒ½ï¼‰
    ax2 = axes[0, 1]
    rel_perfs = [gen_analysis[cond]['mean_relative_performance'] for cond in gen_conditions]
    
    bars = ax2.bar(range(len(gen_conditions)), rel_perfs, alpha=0.7, color='lightgreen')
    ax2.set_title('ç›¸å¯¾æ€§èƒ½\n(æœŸå¾…å€¤ã«å¯¾ã™ã‚‹æ¯”ç‡)', fontweight='bold')
    ax2.set_ylabel('ç›¸å¯¾æ€§èƒ½')
    ax2.set_xticks(range(len(gen_conditions)))
    ax2.set_xticklabels(gen_conditions, rotation=45)
    ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='æœŸå¾…å€¤')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. é©å¿œæˆåŠŸç‡
    ax3 = axes[0, 2]
    adapt_rates = [gen_analysis[cond]['adaptation_success_rate'] for cond in gen_conditions]
    
    bars = ax3.bar(range(len(gen_conditions)), adapt_rates, alpha=0.7, color='lightcoral')
    ax3.set_title('é©å¿œæˆåŠŸç‡\n(ã‚¹ã‚³ã‚¢>100ã®å‰²åˆ)', fontweight='bold')
    ax3.set_ylabel('æˆåŠŸç‡')
    ax3.set_ylim(0, 1)
    ax3.set_xticks(range(len(gen_conditions)))
    ax3.set_xticklabels(gen_conditions, rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # 4. æ±åŒ–åŠ¹æœé‡ï¼ˆæ¨™æº–æ¡ä»¶ã¨ã®æ¯”è¼ƒï¼‰
    ax4 = axes[0, 3]
    effect_sizes = []
    effect_labels = []
    
    for cond in gen_conditions:
        if cond != 'standard' and 'vs_standard' in gen_analysis[cond]:
            effect_sizes.append(gen_analysis[cond]['vs_standard']['cohens_d'])
            effect_labels.append(cond)
    
    colors = ['green' if abs(es) < 0.5 else 'orange' if abs(es) < 0.8 else 'red' for es in effect_sizes]
    bars = ax4.bar(range(len(effect_labels)), effect_sizes, color=colors, alpha=0.7)
    ax4.set_title('æ±åŒ–åŠ¹æœé‡\n(vs æ¨™æº–æ¡ä»¶)', fontweight='bold')
    ax4.set_ylabel('Cohen\'s d')
    ax4.set_xticks(range(len(effect_labels)))
    ax4.set_xticklabels(effect_labels, rotation=45)
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax4.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='ä¸­åŠ¹æœ')
    ax4.axhline(y=-0.5, color='orange', linestyle='--', alpha=0.5)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. æ¸©åº¦åˆ¥æ€§èƒ½
    ax5 = axes[1, 0]
    temp_analysis = abl_analysis['temperature_analysis']
    temp_data = temp_analysis['conditions']
    temp_values = [d['factor_value'] for d in temp_data]
    temp_scores = [d['mean_score'] for d in temp_data]
    
    ax5.plot(temp_values, temp_scores, 'o-', linewidth=2, markersize=8, color='blue')
    ax5.set_title('æ¸©åº¦è¨­å®šã®å½±éŸ¿', fontweight='bold')
    ax5.set_xlabel('æ¸©åº¦')
    ax5.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax5.grid(True, alpha=0.3)
    
    # æœ€é©ç‚¹ã‚’å¼·èª¿
    best_temp = temp_analysis['best_performance']
    ax5.scatter([best_temp['factor_value']], [best_temp['mean_score']], 
               color='red', s=100, zorder=5, label=f'æœ€é©: {best_temp["factor_value"]}')
    ax5.legend()
    
    # 6. ç²’åº¦åˆ¥æ€§èƒ½
    ax6 = axes[1, 1]
    gran_analysis = abl_analysis['granularity_analysis']
    gran_data = gran_analysis['conditions']
    gran_labels = [d['condition'].replace('granularity_', '') for d in gran_data]
    gran_scores = [d['mean_score'] for d in gran_data]
    
    bars = ax6.bar(gran_labels, gran_scores, alpha=0.7, color='lightgreen')
    ax6.set_title('ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç²’åº¦ã®å½±éŸ¿', fontweight='bold')
    ax6.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax6.grid(True, alpha=0.3)
    
    # æœ€é©æ¡ä»¶ã‚’å¼·èª¿
    best_gran = gran_analysis['best_performance']
    best_idx = next(i for i, d in enumerate(gran_data) if d['condition'] == best_gran['condition'])
    bars[best_idx].set_color('darkgreen')
    
    # 7. é »åº¦åˆ¥æ€§èƒ½
    ax7 = axes[1, 2]
    freq_analysis = abl_analysis['frequency_analysis']
    freq_data = freq_analysis['conditions']
    freq_values = [d['factor_value'] for d in freq_data]
    freq_scores = [d['mean_score'] for d in freq_data]
    
    ax7.plot(freq_values, freq_scores, 'o-', linewidth=2, markersize=8, color='purple')
    ax7.set_title('ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é »åº¦ã®å½±éŸ¿', fontweight='bold')
    ax7.set_xlabel('é »åº¦')
    ax7.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax7.grid(True, alpha=0.3)
    
    # æœ€é©ç‚¹ã‚’å¼·èª¿
    best_freq = freq_analysis['best_performance']
    ax7.scatter([best_freq['factor_value']], [best_freq['mean_score']], 
               color='red', s=100, zorder=5, label=f'æœ€é©: {best_freq["factor_value"]}')
    ax7.legend()
    
    # 8. ã‚³ã‚¹ãƒˆåŠ¹ç‡åˆ†æ
    ax8 = axes[1, 3]
    
    # å…¨ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¡ä»¶ã®ã‚³ã‚¹ãƒˆåŠ¹ç‡
    abl_conditions = list(abl_analysis.keys())
    abl_conditions = [c for c in abl_conditions if c.endswith('_analysis') == False]
    
    costs = [abl_analysis[cond]['mean_cost'] for cond in abl_conditions]
    scores = [abl_analysis[cond]['mean_score'] for cond in abl_conditions]
    
    scatter = ax8.scatter(costs, scores, alpha=0.7, s=60)
    ax8.set_title('ã‚³ã‚¹ãƒˆ vs æ€§èƒ½', fontweight='bold')
    ax8.set_xlabel('å¹³å‡APIã‚³ã‚¹ãƒˆ')
    ax8.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax8.grid(True, alpha=0.3)
    
    # ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£ã‚’å¼·èª¿
    cost_eff_values = [abl_analysis[cond]['mean_cost_effectiveness'] for cond in abl_conditions]
    best_cost_eff_idx = np.argmax(cost_eff_values)
    ax8.scatter([costs[best_cost_eff_idx]], [scores[best_cost_eff_idx]], 
               color='red', s=100, zorder=5, label='æœ€é«˜ã‚³ã‚¹ãƒˆåŠ¹ç‡')
    ax8.legend()
    
    # 9. æ¸©åº¦-ç²’åº¦ç›¸äº’ä½œç”¨
    ax9 = axes[2, 0]
    
    # æ¸©åº¦ã¨ç²’åº¦ã®çµ„ã¿åˆã‚ã›åŠ¹æœã‚’å¯è¦–åŒ–
    temp_gran_matrix = np.zeros((4, 3))  # 4æ¸©åº¦ x 3ç²’åº¦
    temp_values_unique = [0.0, 0.3, 0.7, 1.0]
    gran_values_unique = ['step', 'wave', 'event']
    
    for i, temp in enumerate(temp_values_unique):
        for j, gran in enumerate(gran_values_unique):
            # è©²å½“ã™ã‚‹æ¡ä»¶ã‚’æ¢ã™
            matching_conditions = []
            for cond, config in study.ablation_conditions.items():
                if config['temperature'] == temp and config['granularity'] == gran:
                    if cond in abl_analysis:
                        matching_conditions.append(abl_analysis[cond]['mean_score'])
            
            if matching_conditions:
                temp_gran_matrix[i, j] = np.mean(matching_conditions)
    
    im = ax9.imshow(temp_gran_matrix, cmap='viridis', aspect='auto')
    ax9.set_title('æ¸©åº¦-ç²’åº¦ç›¸äº’ä½œç”¨', fontweight='bold')
    ax9.set_xlabel('ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ç²’åº¦')
    ax9.set_ylabel('æ¸©åº¦')
    ax9.set_xticks(range(3))
    ax9.set_xticklabels(gran_values_unique)
    ax9.set_yticks(range(4))
    ax9.set_yticklabels(temp_values_unique)
    plt.colorbar(im, ax=ax9, shrink=0.8)
    
    # 10. æœ€é©è¨­å®šã‚µãƒãƒªãƒ¼
    ax10 = axes[2, 1]
    
    # å„å› å­ã®æœ€é©å€¤ã‚’è¡¨ç¤º
    optimal_settings = {
        'æ¸©åº¦': temp_analysis['best_performance']['factor_value'],
        'ç²’åº¦': gran_analysis['best_performance']['factor_value'],
        'é »åº¦': freq_analysis['best_performance']['factor_value']
    }
    
    optimal_scores = {
        'æ¸©åº¦': temp_analysis['best_performance']['mean_score'],
        'ç²’åº¦': gran_analysis['best_performance']['mean_score'],
        'é »åº¦': freq_analysis['best_performance']['mean_score']
    }
    
    factors = list(optimal_settings.keys())
    scores = [optimal_scores[f] for f in factors]
    
    bars = ax10.bar(factors, scores, alpha=0.7, color=['blue', 'green', 'purple'])
    ax10.set_title('å„å› å­ã®æœ€é©è¨­å®šæ€§èƒ½', fontweight='bold')
    ax10.set_ylabel('æœ€é©æ¡ä»¶ã§ã®å¹³å‡ã‚¹ã‚³ã‚¢')
    ax10.grid(True, alpha=0.3)
    
    # æœ€é©å€¤ã‚’è¡¨ç¤º
    for i, (factor, bar) in enumerate(zip(factors, bars)):
        height = bar.get_height()
        optimal_val = optimal_settings[factor]
        ax10.text(bar.get_x() + bar.get_width()/2., height + 5,
                 f'{optimal_val}', ha='center', va='bottom', fontweight='bold')
    
    # 11. æ±åŒ–æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    ax11 = axes[2, 2]
    
    # æ±åŒ–æ¡ä»¶ã‚’ç›¸å¯¾æ€§èƒ½ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    gen_ranking = [(cond, gen_analysis[cond]['mean_relative_performance']) 
                   for cond in gen_conditions]
    gen_ranking.sort(key=lambda x: x[1], reverse=True)
    
    rank_labels = [f"{i+1}. {cond}" for i, (cond, _) in enumerate(gen_ranking)]
    rank_values = [perf for _, perf in gen_ranking]
    
    bars = ax11.barh(range(len(rank_labels)), rank_values, alpha=0.7, color='lightblue')
    ax11.set_title('æ±åŒ–æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n(ç›¸å¯¾æ€§èƒ½é †)', fontweight='bold')
    ax11.set_xlabel('ç›¸å¯¾æ€§èƒ½')
    ax11.set_yticks(range(len(rank_labels)))
    ax11.set_yticklabels(rank_labels)
    ax11.axvline(x=1.0, color='red', linestyle='--', alpha=0.7, label='æœŸå¾…å€¤')
    ax11.legend()
    ax11.grid(True, alpha=0.3)
    
    # 12. ç·åˆè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    ax12 = axes[2, 3]
    
    # æ€§èƒ½ã€ã‚³ã‚¹ãƒˆã€æ±åŒ–æ€§ã®ç·åˆè©•ä¾¡
    evaluation_data = []
    
    # æ¨™æº–LLMæ•™å¸«ã®æ€§èƒ½ã‚’åŸºæº–ã¨ã™ã‚‹
    standard_score = gen_analysis['standard']['mean_score']
    
    # ä¸»è¦ãªæ±åŒ–æ¡ä»¶ã®è©•ä¾¡
    key_gen_conditions = ['standard', 'large_map', 'low_resources', 'hard_waves']
    
    for cond in key_gen_conditions:
        if cond in gen_analysis:
            score_ratio = gen_analysis[cond]['mean_score'] / standard_score
            adaptation_rate = gen_analysis[cond]['adaptation_success_rate']
            
            evaluation_data.append({
                'condition': cond,
                'performance': score_ratio,
                'adaptation': adaptation_rate,
                'robustness': 1 - abs(1 - score_ratio)  # 1ã«è¿‘ã„ã»ã©é ‘å¥
            })
    
    if evaluation_data:
        conditions = [d['condition'] for d in evaluation_data]
        performance = [d['performance'] for d in evaluation_data]
        adaptation = [d['adaptation'] for d in evaluation_data]
        robustness = [d['robustness'] for d in evaluation_data]
        
        x = np.arange(len(conditions))
        width = 0.25
        
        ax12.bar(x - width, performance, width, label='æ€§èƒ½æ¯”', alpha=0.7, color='blue')
        ax12.bar(x, adaptation, width, label='é©å¿œç‡', alpha=0.7, color='green')
        ax12.bar(x + width, robustness, width, label='é ‘å¥æ€§', alpha=0.7, color='orange')
        
        ax12.set_title('ç·åˆè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontweight='bold')
        ax12.set_ylabel('è©•ä¾¡å€¤')
        ax12.set_xticks(x)
        ax12.set_xticklabels(conditions, rotation=45)
        ax12.legend()
        ax12.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    output_path = '/home/ubuntu/tower-defense-llm/generalization_ablation_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Tower Defense ELM - æ±åŒ–ãƒ†ã‚¹ãƒˆ & ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶")
    print("=" * 80)
    
    # å®Ÿé¨“å®Ÿè¡Œ
    study = GeneralizationAblationStudy()
    
    # æ±åŒ–ãƒ†ã‚¹ãƒˆ
    study.run_generalization_tests()
    gen_analysis = study.analyze_generalization_results()
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
    study.run_ablation_studies()
    abl_analysis = study.analyze_ablation_results()
    
    # å¯è¦–åŒ–ä½œæˆ
    viz_path = create_generalization_ablation_visualization(study, gen_analysis, abl_analysis)
    print(f"ğŸ¨ å¯è¦–åŒ–ä½œæˆå®Œäº†: {viz_path}")
    
    # çµæœä¿å­˜
    output_data = {
        'generalization_results': study.results['generalization'],
        'ablation_results': study.results['ablation'],
        'generalization_analysis': gen_analysis,
        'ablation_analysis': abl_analysis,
        'metadata': study.results['metadata']
    }
    
    json_path = '/home/ubuntu/tower-defense-llm/generalization_ablation_results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
    print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†: {json_path}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ±åŒ–ãƒ†ã‚¹ãƒˆ & ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
    # ä¸»è¦çµæœã®ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“‹ æ±åŒ–ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   æœ€é«˜æ€§èƒ½: {max(gen_analysis.keys(), key=lambda k: gen_analysis[k]['mean_score'])}")
    print(f"   æœ€é«˜é©å¿œç‡: {max(gen_analysis.keys(), key=lambda k: gen_analysis[k]['adaptation_success_rate'])}")
    
    print(f"\nğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶çµæœ:")
    print(f"   æœ€é©æ¸©åº¦: {abl_analysis['temperature_analysis']['best_performance']['factor_value']}")
    print(f"   æœ€é©ç²’åº¦: {abl_analysis['granularity_analysis']['best_performance']['factor_value']}")
    print(f"   æœ€é©é »åº¦: {abl_analysis['frequency_analysis']['best_performance']['factor_value']}")

if __name__ == "__main__":
    main()
